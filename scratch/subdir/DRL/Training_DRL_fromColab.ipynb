{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhMJBREMIVmI"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'env' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n env ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import csv\n",
        "# import drl_utils as dr\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import select\n",
        "import torch as T\n",
        "from torch.distributions.utils import logits_to_probs\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal, Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "P1VPX8yMIXnG"
      },
      "outputs": [],
      "source": [
        "class CustomObservationSpace(spaces.Dict):\n",
        "    def __init__(self, total_nodes):\n",
        "        self.total_nodes = total_nodes\n",
        "        # Define the min and max values for each feature\n",
        "        low_freq = 50.0\n",
        "        high_freq = 300.0\n",
        "        low_rate = 150.0\n",
        "        high_rate = 1000.0\n",
        "        min_honesty = -500.0\n",
        "        max_honesty = 500.0\n",
        "        min_data = 100.0\n",
        "        max_data = 1000.0\n",
        "        feature_min_values = np.array([0,0.0, min_honesty, min_data, low_freq, low_rate, 0.0,0.0], dtype=np.float32)\n",
        "        feature_max_values = np.array([total_nodes,1.0, max_honesty, max_data, high_freq, high_rate,1.0 ,100.0], dtype=np.float32)\n",
        "        observation_low = np.tile(feature_min_values, (total_nodes, 1))\n",
        "        observation_high = np.tile(feature_max_values, (total_nodes, 1))\n",
        "\n",
        "        current_state_space = spaces.Box(low=observation_low, high=observation_high, dtype=np.float32)\n",
        "\n",
        "        observation_space_dict = spaces.Dict(\n",
        "            {\n",
        "                \"current_state\":current_state_space, # tableau de noeuds with features\n",
        "                \"FL_accuracy\": spaces.Box(low=0.0, high=1.0, dtype=np.float32),\n",
        "            }\n",
        "        )\n",
        "        super().__init__(observation_space_dict) # type : ignore\n",
        "    def sample(self):\n",
        "        obs= super().sample()\n",
        "        indexes = np.arange(obs[\"current_state\"].shape[0])\n",
        "\n",
        "        # Replace the first column of current_state with the indexes\n",
        "        obs[\"current_state\"][:, 0] = indexes\n",
        "        return obs\n",
        "    def preprocess_observation(self, current_state, fl_accuracy):\n",
        "        flattened_current_state = current_state.flatten()\n",
        "        processed_observation = np.hstack((flattened_current_state, fl_accuracy))\n",
        "\n",
        "        return processed_observation\n",
        "\n",
        "class CustomActionSpace(spaces.Space):\n",
        "    def __init__(self, total_nodes, num_selected):\n",
        "        self.total_nodes = total_nodes\n",
        "        self.num_selected = num_selected\n",
        "        self.high = np.ones(self.num_selected)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.total_nodes,)\n",
        "\n",
        "    def sample(self):\n",
        "        action = np.random.choice(self.total_nodes, size=self.num_selected, replace=False)\n",
        "        result = np.zeros(self.total_nodes)\n",
        "        for i in action :\n",
        "            result[i] = 1\n",
        "        return result, action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asXTYMKAIYR6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'env' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n env ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "class FLNodeSelectionEnv(gym.Env):\n",
        "    def __init__(self,total_nodes,num_selected , num_features,target,max_rounds,aggregator_ratio=0.3):\n",
        "        super().__init__()\n",
        "        self.total_nodes = total_nodes\n",
        "        self.num_selected = num_selected\n",
        "        self.num_features = num_features\n",
        "        self.aggregator_ratio = aggregator_ratio\n",
        "        self.target_accuracy = target\n",
        "        # Calculate the number of aggregators and trainers based on the ratio\n",
        "        num_aggregators = int(num_selected * aggregator_ratio)\n",
        "        num_trainers = num_selected - num_aggregators\n",
        "        self.num_aggregators = num_aggregators\n",
        "        self.num_trainers = num_trainers\n",
        "        self.current_state =  np.zeros((total_nodes, num_features+1)) # room for id and node accuracy\n",
        "        self.current_state[:, 0] = np.arange(total_nodes)  # Set node IDs\n",
        "        self.observation_space = CustomObservationSpace(total_nodes)\n",
        "        self.action_space = CustomActionSpace(total_nodes, num_selected)\n",
        "        # setting the initial state\n",
        "        self.fl_accuracy = 0.0\n",
        "        self.current_observation = {\n",
        "                \"current_state\":self.current_state,\n",
        "                \"FL_accuracy\": self.fl_accuracy}\n",
        "\n",
        "        self.current_round=0\n",
        "        self.max_rounds: int =max_rounds\n",
        "\n",
        "    def set_act(self, act):\n",
        "        self.act = act\n",
        "        \n",
        "    def reset(self): #CALLED TO INITIATE NEW EPISODE\n",
        "        #should return the observation of the initial state\n",
        "        # We need the following line to seed self.np_random\n",
        "        csv_filename = \"generated_nodes.csv\"  # Replace with your CSV file name\n",
        "        with open(csv_filename, \"r\") as csv_file:\n",
        "            csv_reader = csv.reader(csv_file)\n",
        "            rows = list(csv_reader)\n",
        "        # Extract the observations from the CSV rows\n",
        "        current_state_rows = rows[1:self.total_nodes+1] # has true and false in it we remove first row having the name of the features\n",
        "        #prepocessing the data from csv to change Bool to int\n",
        "        current_state_preprocessed = []\n",
        "        for row in current_state_rows:\n",
        "            preprocessed_row = []\n",
        "            for value in row[:self.num_features]:\n",
        "                if value == \"true\":\n",
        "                    preprocessed_row.append(1)\n",
        "                elif value == \"false\":\n",
        "                    preprocessed_row.append(0)\n",
        "                else:\n",
        "                    preprocessed_row.append(value)  # Keep other values unchanged\n",
        "            current_state_preprocessed.append(preprocessed_row)\n",
        "        # Convert the preprocessed rows to a NumPy array\n",
        "        new_current_state = np.array(current_state_preprocessed, dtype=np.float32)[:, :self.num_features+1]\n",
        "        new_column = np.zeros((new_current_state.shape[0], 1))\n",
        "        # Append the new column to the existing array\n",
        "        current_state = np.append(new_current_state, new_column, axis=1) #added the accuracy of local model column\n",
        "\n",
        "        self.current_state[:self.total_nodes] = current_state\n",
        "        # Create initial values for other parts of the observation\n",
        "        current_observation = {\n",
        "            \"current_state\": current_state,\n",
        "            \"FL_accuracy\": 0.0\n",
        "        }\n",
        "        self._observation = current_observation\n",
        "        info = {\"msg\" : \"success\"}\n",
        "        return current_observation , info\n",
        "\n",
        "    def step(self, action, accuracies, nodes, losses, fl_accuracy):\n",
        "        updated_nodes =dr.get_nodes_withaccuracy(nodes, self.total_nodes,accuracies)\n",
        "        updated_fl_accuracy =fl_accuracy\n",
        "\n",
        "        # Update the state of the environment with received updates\n",
        "        next_observation = self.update_environment_state_with_network_updates(updated_nodes, fl_accuracy)\n",
        "\n",
        "        self.current_observation = next_observation\n",
        "        # Simulate FL round and get rewards\n",
        "        node_rewards = self.calculate_reward(action,accuracies)\n",
        "        agent_reward = sum(node_rewards)# or agent_reward = self.agent_reward(node_rewards) in case we change the way we calcultae the agent reward\n",
        "        # Update the state of the environment\n",
        "        self.current_round += 1\n",
        "        # Check if the maximum number of rounds is reached or the target accuracy is achieved\n",
        "        done = self.current_round >= self.max_rounds or self.target_accuracy_achieved(updated_fl_accuracy)\n",
        "\n",
        "        return next_observation, agent_reward,done, node_rewards\n",
        "\n",
        "\n",
        "    def update_environment_state_with_network_updates(self,nodes,FL_accuracy):\n",
        "        # Update the state of the environment with received updates\n",
        "        #check the shape of nodes if it includes accuracy\n",
        "        nodes= nodes.astype(np.float32) # cast the array to accept npfloat\n",
        "        obs= {\n",
        "            \"current_state\": nodes,\n",
        "            \"FL_accuracy\": FL_accuracy\n",
        "        }\n",
        "        self.current_observation = obs\n",
        "        return obs\n",
        "\n",
        "\n",
        "    def agent_reward(self, node_rewards):\n",
        "        return sum(node_rewards)\n",
        "\n",
        "    def calculate_reward(self, selected_nodes, updated_accuracies):\n",
        "        #todo to change to accuracy\n",
        "        # print(\"nodes in reward\", self.current_observation)\n",
        "        # print(\"selected nodes\",  selected_nodes)\n",
        "        state = self.current_observation[\"current_state\"]\n",
        "        node_rewards = np.zeros(self.total_nodes)\n",
        "        # print (\"updated_losses\", updated_losses)\n",
        "        for node_index in selected_nodes:\n",
        "            node_rewards[node_index] = state[node_index][2]\n",
        "            if (updated_accuracies[node_index] != 0) :\n",
        "                node_rewards[node_index] *= updated_accuracies[node_index]  # Use loss as a simple example because loss is positive\n",
        "            # print(\"node reward\", node_rewards[node_index])\n",
        "        return node_rewards\n",
        "    def target_accuracy_achieved(self, updated_accuracy):\n",
        "        return updated_accuracy >= self.target_accuracy\n",
        "\n",
        "    # def __render__(self, mode=\"human\"):\n",
        "    #     #should render the environment\n",
        "    #     pass\n",
        "\n",
        "    # def __close__(self):\n",
        "    #     #should close the environment\n",
        "    #     pass\n",
        "\n",
        "    # def __seed__(self, seed=None):\n",
        "    #     #should set the seed for this env's random number generator(s)\n",
        "    #     pass\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return self.current_observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "pa0fcHQ7JUTj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReplayBuffer():\n",
        "    #max_size is max memory,n_actions number of actions, input_shape is observation shape\n",
        "    def __init__(self,max_size,input_shape,n_actions, max_action):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0 # memory counter to keep track\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size,*input_shape)) #state memory\n",
        "        self.new_state_memory = np.zeros((self.mem_size,*input_shape)) #new state memory\n",
        "        self.action_memory = np.zeros((self.mem_size,max_action)) #action memory\n",
        "        self.reward_memory = np.zeros(self.mem_size) #reward memory\n",
        "        self.terminal_memory = np.zeros(self.mem_size,dtype=bool) #terminal memory we need it to store the done flags\n",
        "\n",
        "    def preprocess_observation(self, observation):\n",
        "        current_state = observation['current_state']\n",
        "        fl_accuracy = observation['FL_accuracy']\n",
        "\n",
        "        # Flatten the current_state component\n",
        "        flattened_current_state = current_state.flatten()\n",
        "\n",
        "        # Concatenate the flattened current_state and the FL_accuracy\n",
        "        processed_observation = np.concatenate((flattened_current_state, [fl_accuracy]))\n",
        "\n",
        "        return processed_observation\n",
        "\n",
        "    def store_transition(self,state,action,reward,state_,done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self,batch_size):\n",
        "        max_mem = min(self.mem_cntr,self.mem_size)\n",
        "        batch = np.random.choice(max_mem,batch_size) #randomly choose the batch size from the memory\n",
        "        states = self.state_memory[batch]#get the states\n",
        "        states_ = self.new_state_memory[batch]#get the new states\n",
        "        actions = self.action_memory[batch] #get the actions\n",
        "        rewards = self.reward_memory[batch] #get the rewards\n",
        "        dones = self.terminal_memory[batch] #get the done flags\n",
        "        return states[0], actions[0], rewards[0], states_[0], dones[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh_gChIIzUVI",
        "outputId": "7cab2ae5-9eaf-4d87-9a75-745b5781f998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global Accuracy Values: [60.0, 73.53, 74.71, 83.0, 83.0, 85.0, 86.0, 87.0, 85.0, 90.0, 88.0, 89.0, 87.0, 89.69, 89.84, 92.0, 92.0, 93.0, 93.0, 91.88, 92.0, 93.0, 94.24, 91.21, 93.27, 93.0, 91.76, 91.0, 93.45]\n"
          ]
        }
      ],
      "source": [
        "# Create a list to store the global accuracy values\n",
        "global_accuracy = []\n",
        "\n",
        "# Iterate through the transactions of all blocks\n",
        "for json_obj in data:\n",
        "    block_id = json_obj['BlockId']\n",
        "\n",
        "    # Find the transaction with message_type equal to 2 and type equal to 2\n",
        "    accuracy_transaction = None\n",
        "    for transaction in json_obj['Transactions']:\n",
        "        content = transaction['Content']\n",
        "        if content.get('message_type') == 2 and content.get('type') == 2:\n",
        "            accuracy_transaction = transaction\n",
        "            break\n",
        "\n",
        "    if accuracy_transaction is not None:\n",
        "        accuracy = accuracy_transaction['Content'].get('accuracy', 0.0)\n",
        "        global_accuracy.append(accuracy)\n",
        "\n",
        "# Print or use global_accuracy as needed\n",
        "print(\"Global Accuracy Values:\", global_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etp1hy-YzL3S",
        "outputId": "9cb6d9ac-14c9-4854-de2b-8538759fe11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block 1/0: [32, 1, 7, 30, 35, 13, 42, 6, 4, 27, 14, 36, 16, 38, 3, 15, 23]\n",
            "Block 1/1: [37, 1, 7, 30, 35, 46, 19, 8, 13, 42, 43, 6, 4, 49, 27, 36, 16]\n",
            "Block 1/2: [37, 30, 35, 19, 42, 49, 27, 1, 36, 16, 7, 38, 3, 12, 15, 23, 9]\n",
            "Block 1/3: [32, 37, 30, 35, 19, 42, 13, 43, 27, 4, 36, 12, 15, 23, 9, 26, 5]\n",
            "Block 1/4: [32, 37, 30, 35, 19, 42, 46, 13, 43, 27, 49, 4, 36, 16, 38, 12, 15]\n",
            "Block 1/5: [32, 30, 19, 46, 13, 43, 42, 49, 36, 16, 38, 15, 26, 29, 2, 45, 48]\n",
            "Block 1/6: [32, 30, 19, 35, 46, 13, 42, 49, 27, 38, 15, 23, 26, 29, 45, 48, 6]\n",
            "Block 1/7: [32, 30, 37, 19, 35, 46, 13, 42, 43, 49, 27, 16, 38, 15, 23, 26, 29]\n",
            "Block 1/8: [32, 30, 37, 19, 35, 46, 13, 42, 43, 49, 27, 36, 16, 38, 15, 12, 23]\n",
            "Block 1/9: [32, 37, 30, 19, 35, 46, 13, 42, 43, 27, 36, 16, 38, 15, 12, 23, 26]\n",
            "Block 1/10: [32, 37, 30, 19, 35, 46, 13, 43, 27, 16, 38, 15, 12, 23, 26, 29, 45]\n",
            "Block 1/11: [32, 37, 30, 46, 13, 43, 42, 49, 16, 38, 36, 12, 23, 26, 29, 45, 48]\n",
            "Block 1/12: [32, 30, 19, 46, 13, 42, 27, 16, 38, 36, 15, 12, 23, 26, 45, 0, 1]\n",
            "Block 1/13: [30, 37, 19, 46, 13, 35, 43, 49, 16, 38, 36, 15, 12, 23, 26, 29, 45]\n",
            "Block 1/14: [30, 32, 19, 46, 13, 35, 42, 27, 16, 38, 36, 12, 23, 26, 45, 48, 34]\n",
            "Block 1/15: [32, 37, 46, 35, 42, 27, 16, 49, 38, 15, 12, 23, 29, 48, 34, 7, 47]\n",
            "Block 1/16: [32, 30, 37, 46, 35, 13, 42, 43, 27, 16, 49, 38, 36, 15, 12, 26, 29]\n",
            "Block 1/17: [32, 46, 19, 35, 13, 42, 27, 49, 38, 36, 12, 26, 34, 33, 47, 39, 9]\n",
            "Block 1/18: [32, 46, 30, 19, 37, 35, 13, 42, 27, 43, 16, 49, 38, 12, 15, 23, 26]\n",
            "Block 1/19: [30, 37, 35, 13, 42, 27, 16, 49, 38, 36, 23, 26, 45, 48, 34, 33, 47]\n",
            "Block 1/20: [32, 30, 46, 35, 37, 19, 42, 27, 49, 38, 36, 12, 23, 26, 29, 45, 48]\n",
            "Block 1/21: [32, 46, 35, 37, 19, 13, 42, 43, 38, 36, 12, 23, 15, 26, 29, 45, 48]\n",
            "Block 1/22: [46, 35, 37, 19, 13, 42, 27, 43, 49, 36, 12, 23, 15, 26, 29, 45, 34]\n",
            "Block 1/23: [46, 37, 32, 19, 30, 13, 27, 43, 49, 38, 36, 12, 23, 15, 26, 29, 45]\n",
            "Block 1/24: [19, 32, 37, 30, 13, 42, 27, 43, 49, 36, 16, 12, 23, 26, 45, 34, 33]\n",
            "Block 1/25: [32, 37, 19, 30, 13, 35, 42, 27, 38, 36, 12, 23, 15, 29, 45, 48, 33]\n",
            "Block 1/26: [37, 19, 32, 30, 35, 46, 42, 27, 43, 38, 36, 16, 12, 23, 15, 26, 29]\n",
            "Block 1/27: [32, 19, 30, 35, 46, 13, 42, 27, 43, 38, 49, 16, 12, 23, 15, 26, 29]\n",
            "Block 1/28: [19, 32, 30, 37, 35, 46, 42, 27, 43, 38, 49, 36, 16, 23, 26, 29, 45]\n",
            "Block 1/29: [32, 19, 30, 37, 46, 13, 35, 42, 27, 43, 38, 49, 36, 23, 12, 26, 15]\n"
          ]
        }
      ],
      "source": [
        "# Create a list to store the concatenated lists for each block\n",
        "selected_nodes = []\n",
        "\n",
        "# Iterate through the transactions of all blocks\n",
        "for json_obj in data:\n",
        "    block_id = json_obj['BlockId']\n",
        "\n",
        "    # Find the transaction with message_type equal to 4\n",
        "    aggregator_transaction = None\n",
        "    for transaction in json_obj['Transactions']:\n",
        "        if transaction['Content'].get('message_type') == 4:\n",
        "            aggregator_transaction = transaction\n",
        "            break\n",
        "\n",
        "    if aggregator_transaction is not None:\n",
        "        aggregators = aggregator_transaction['Content'].get('aggregators', [])\n",
        "        trainers = aggregator_transaction['Content'].get('trainers', [])\n",
        "\n",
        "        selected_nodes.append(aggregators + trainers)\n",
        "\n",
        "# Print or use selected_nodes as needed\n",
        "for block_id, selected_node_list in zip([json_obj['BlockId'] for json_obj in data], selected_nodes):\n",
        "    print(f\"Block {block_id}: {selected_node_list}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFXbkllvzGIa",
        "outputId": "ea46094a-8c95-4611-f237-006dead5ee2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, 0.0, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, 0.0, 450, 180.0, 450.0, 0.0, 56.67], [4, 1, 0.0, 800, 280.0, 640.0, 0.0, 68.75], [5, 1, 0.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, 0.0, 760, 240.0, 830.0, 0.0, 69.74], [7, 1, 0.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, 0.0, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, 0.0, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 0.0, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 0.0, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, 0.0, 700, 180.0, 890.0, 0.0, 72.86], [15, 1, 0.0, 830, 270.0, 470.0, 0.0, 63.25], [16, 1, 0.0, 450, 150.0, 550.0, 0.0, 63.33], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 0.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 0.0, 700, 160.0, 730.0, 0.0, 69.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.0, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 0.0, 890, 260.0, 750.0, 0.0, 70.22], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 0.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 0.0, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 0.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 0.0, 950, 230.0, 930.0, 0.0, 64.74], [37, 0, 0.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 0.0, 760, 190.0, 780.0, 0.0, 71.05], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 0.0, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 0.0, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 0, 0.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 0.0, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, 1.8, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, 0.956, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, 0.943, 800, 280.0, 640.0, 0.0, 70.0], [5, 1, 0.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, 0.973, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, 2.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, 0.0, 380, 260.0, 540.0, 0.0, 86.84], [9, 1, 0.0, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.0, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 2.0, 840, 300.0, 750.0, 0.0, 72.62], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 0.972, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 0.946, 450, 150.0, 550.0, 0.0, 72.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 0.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 0.939, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.0, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 0.935, 890, 260.0, 750.0, 0.0, 72.47], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 2.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 2.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 0.932, 950, 230.0, 930.0, 0.0, 68.42], [37, 1, 0.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 0.95, 760, 190.0, 780.0, 0.0, 0.0], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.0, 310, 250.0, 430.0, 0.0, 72.58], [43, 1, 0.0, 450, 250.0, 590.0, 0.0, 71.11], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 0.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 0.0, 420, 150.0, 640.0, 0.0, 72.62]], 'action': [32, 1, 7, 30, 35, 13, 42, 6, 4, 27, 14, 36, 16, 38, 3, 15, 23], 'reward': 0.0, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, 1.8, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, 0.956, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, 0.943, 800, 280.0, 640.0, 0.0, 70.0], [5, 1, 0.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, 0.973, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, 2.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, 0.0, 380, 260.0, 540.0, 0.0, 86.84], [9, 1, 0.0, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.0, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 2.0, 840, 300.0, 750.0, 0.0, 72.62], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 0.972, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 0.946, 450, 150.0, 550.0, 0.0, 72.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 0.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 0.939, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.0, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 0.935, 890, 260.0, 750.0, 0.0, 72.47], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 2.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 2.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 0.932, 950, 230.0, 930.0, 0.0, 68.42], [37, 1, 0.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 0.95, 760, 190.0, 780.0, 0.0, 0.0], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.0, 310, 250.0, 430.0, 0.0, 72.58], [43, 1, 0.0, 450, 250.0, 590.0, 0.0, 71.11], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 0.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 0.0, 420, 150.0, 640.0, 0.0, 72.62]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -8.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, 0.956, 450, 180.0, 450.0, 0.0, 82.67], [4, 0, 1.908, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, 0.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -8.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, 0.0, 300, 60.0, 990.0, 0.0, 68.33], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.0, 520, 200.0, 460.0, 0.0, 67.31], [13, 0, 2.965, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 0.972, 830, 270.0, 470.0, 0.0, 69.28], [16, 1, 1.926, 450, 150.0, 550.0, 0.0, 77.78], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 2.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 0.939, 700, 160.0, 730.0, 0.0, 77.86], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 0.0, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 1.908, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 4.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 4.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 1.896, 950, 230.0, 930.0, 0.0, 71.58], [37, 1, 1.444, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 0.95, 760, 190.0, 780.0, 0.0, 74.34], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 0.974, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 0, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 0.974, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [37, 1, 7, 30, 35, 46, 19, 8, 13, 42, 43, 6, 4, 49, 27, 36, 16], 'reward': 565.0300100000001, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -8.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, 0.956, 450, 180.0, 450.0, 0.0, 82.67], [4, 0, 1.908, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, 0.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -8.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, 0.0, 300, 60.0, 990.0, 0.0, 68.33], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.0, 520, 200.0, 460.0, 0.0, 67.31], [13, 0, 2.965, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 0.972, 830, 270.0, 470.0, 0.0, 69.28], [16, 1, 1.926, 450, 150.0, 550.0, 0.0, 77.78], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 2.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 0.939, 700, 160.0, 730.0, 0.0, 77.86], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 0.0, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 1.908, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 4.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 4.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 1.896, 950, 230.0, 930.0, 0.0, 71.58], [37, 1, 1.444, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 0.95, 760, 190.0, 780.0, 0.0, 74.34], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 0.974, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 0, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 0.974, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, 1.908, 800, 280.0, 640.0, 0.0, 80.0], [5, 1, 0.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, 0.981, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.986, 520, 200.0, 460.0, 0.0, 70.19], [13, 1, 2.965, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 1.941, 830, 270.0, 470.0, 0.0, 71.08], [16, 0, 2.904, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 4.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 1.913, 700, 160.0, 730.0, 0.0, 81.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.0, 890, 230.0, 590.0, 0.0, 82.58], [27, 1, 3.908, 890, 260.0, 750.0, 0.0, 74.16], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 6.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 6.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 2.868, 950, 230.0, 930.0, 0.0, 79.47], [37, 1, 3.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 1.934, 760, 190.0, 780.0, 0.0, 0.0], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 4.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 0.974, 450, 250.0, 590.0, 0.0, 76.67], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 0, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 2.974, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [37, 30, 35, 19, 42, 49, 27, 1, 36, 16, 7, 38, 3, 12, 15, 23, 9], 'reward': 576.74518, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, 1.908, 800, 280.0, 640.0, 0.0, 80.0], [5, 1, 0.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, 0.981, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 0.986, 520, 200.0, 460.0, 0.0, 70.19], [13, 1, 2.965, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 1.941, 830, 270.0, 470.0, 0.0, 71.08], [16, 0, 2.904, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 4.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 1.913, 700, 160.0, 730.0, 0.0, 81.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.0, 890, 230.0, 590.0, 0.0, 82.58], [27, 1, 3.908, 890, 260.0, 750.0, 0.0, 74.16], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 6.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 1.667, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 6.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 2.868, 950, 230.0, 930.0, 0.0, 79.47], [37, 1, 3.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 1.934, 760, 190.0, 780.0, 0.0, 0.0], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 4.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 0.974, 450, 250.0, 590.0, 0.0, 76.67], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 0, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 2.974, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, 2.876, 800, 280.0, 640.0, 0.0, 99.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 1.957, 520, 200.0, 460.0, 0.0, 74.04], [13, 1, 4.965, 840, 300.0, 750.0, 0.0, 85.71], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 2.906, 830, 270.0, 470.0, 0.0, 74.7], [16, 1, 2.904, 450, 150.0, 550.0, 0.0, 81.11], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 6.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 2.905, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 0.974, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 4.874, 890, 260.0, 750.0, 0.0, 78.09], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 8.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 3.111, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 8.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 3.844, 950, 230.0, 930.0, 0.0, 80.0], [37, 1, 5.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 1.934, 760, 190.0, 780.0, 0.0, 73.68], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 6.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 1.947, 450, 250.0, 590.0, 0.0, 85.56], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 2.974, 420, 150.0, 640.0, 0.0, 82.14]], 'action': [32, 37, 30, 35, 19, 42, 13, 43, 27, 4, 36, 12, 15, 23, 9, 26, 5], 'reward': 1137.6090299999998, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, 2.876, 800, 280.0, 640.0, 0.0, 99.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 1.957, 520, 200.0, 460.0, 0.0, 74.04], [13, 1, 4.965, 840, 300.0, 750.0, 0.0, 85.71], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 2.906, 830, 270.0, 470.0, 0.0, 74.7], [16, 1, 2.904, 450, 150.0, 550.0, 0.0, 81.11], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 6.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 2.905, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 0.974, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 4.874, 890, 260.0, 750.0, 0.0, 78.09], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 8.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 3.111, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 8.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 3.844, 950, 230.0, 930.0, 0.0, 80.0], [37, 1, 5.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 1.934, 760, 190.0, 780.0, 0.0, 73.68], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 6.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 1.947, 450, 250.0, 590.0, 0.0, 85.56], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 0.0, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 2.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 0.0, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 2.974, 420, 150.0, 640.0, 0.0, 82.14]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 96.47], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 5.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 3.874, 830, 270.0, 470.0, 0.0, 77.71], [16, 1, 3.879, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 8.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 2.905, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.974, 890, 230.0, 590.0, 0.0, 84.27], [27, 0, 5.844, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 85.71], [30, 1, 10.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 4.611, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 10.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 4.818, 950, 230.0, 930.0, 0.0, 82.11], [37, 0, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 2.92, 760, 190.0, 780.0, 0.0, 75.66], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 2.922, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 4.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 86.49], [49, 1, 3.95, 420, 150.0, 640.0, 0.0, 84.52]], 'action': [32, 37, 30, 35, 19, 42, 46, 13, 43, 27, 49, 4, 36, 16, 38, 12, 15], 'reward': 2588.39353, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, 0.0, 510, 100.0, 730.0, 0.0, 96.47], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -9.027, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 5.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 3.874, 830, 270.0, 470.0, 0.0, 77.71], [16, 1, 3.879, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 8.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 2.905, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 0.974, 890, 230.0, 590.0, 0.0, 84.27], [27, 0, 5.844, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.0, 140, 140.0, 270.0, 0.0, 85.71], [30, 1, 10.0, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 4.611, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 10.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 4.818, 950, 230.0, 930.0, 0.0, 82.11], [37, 0, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 2.92, 760, 190.0, 780.0, 0.0, 75.66], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 2.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 2.922, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.0, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 4.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.0, 740, 130.0, 930.0, 0.0, 86.49], [49, 1, 3.95, 420, 150.0, 640.0, 0.0, 84.52]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 97.89], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 7.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 4.847, 830, 270.0, 470.0, 0.0, 77.71], [16, 0, 4.859, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 10.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 2.905, 700, 160.0, 730.0, 0.0, 88.57], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 1.941, 890, 230.0, 590.0, 0.0, 82.02], [27, 1, 5.844, 890, 260.0, 750.0, 0.0, 80.9], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.99, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 11.75, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 6.211, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 10.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 5.792, 950, 230.0, 930.0, 0.0, 0.0], [37, 0, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 3.893, 760, 190.0, 780.0, 0.0, 82.89], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 4.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 4.922, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.989, 410, 80.0, 710.0, 0.0, 79.27], [46, 1, 6.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.987, 740, 130.0, 930.0, 0.0, 83.78], [49, 1, 4.931, 420, 150.0, 640.0, 0.0, 83.33]], 'action': [32, 30, 19, 46, 13, 43, 42, 49, 36, 16, 38, 15, 26, 29, 2, 45, 48], 'reward': 1690.9220799999998, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -9.027, 760, 240.0, 830.0, 0.0, 97.89], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 7.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 4.847, 830, 270.0, 470.0, 0.0, 77.71], [16, 0, 4.859, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 10.0, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 2.905, 700, 160.0, 730.0, 0.0, 88.57], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 1.941, 890, 230.0, 590.0, 0.0, 82.02], [27, 1, 5.844, 890, 260.0, 750.0, 0.0, 80.9], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 0.99, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 11.75, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 6.211, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 10.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 5.792, 950, 230.0, 930.0, 0.0, 0.0], [37, 0, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 3.893, 760, 190.0, 780.0, 0.0, 82.89], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 4.993, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 4.922, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 0.989, 410, 80.0, 710.0, 0.0, 79.27], [46, 1, 6.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 0.987, 740, 130.0, 930.0, 0.0, 83.78], [49, 1, 4.931, 420, 150.0, 640.0, 0.0, 83.33]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 9.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 5.818, 830, 270.0, 470.0, 0.0, 76.51], [16, 1, 4.859, 450, 150.0, 550.0, 0.0, 83.33], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 11.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 3.882, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 2.907, 890, 230.0, 590.0, 0.0, 86.52], [27, 1, 6.817, 890, 260.0, 750.0, 0.0, 79.21], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 1.981, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 13.55, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 7.989, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 11.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 5.792, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 4.872, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 6.993, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 4.922, 450, 250.0, 590.0, 0.0, 86.67], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 8.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 5.912, 420, 150.0, 640.0, 0.0, 83.33]], 'action': [32, 30, 19, 35, 46, 13, 42, 49, 27, 38, 15, 23, 26, 29, 45, 48, 6], 'reward': 1411.6517999999996, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 2.934, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 9.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 5.818, 830, 270.0, 470.0, 0.0, 76.51], [16, 1, 4.859, 450, 150.0, 550.0, 0.0, 83.33], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 11.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 3.882, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 2.907, 890, 230.0, 590.0, 0.0, 86.52], [27, 1, 6.817, 890, 260.0, 750.0, 0.0, 79.21], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 1.981, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 13.55, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 7.989, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 11.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 5.792, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 7.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 4.872, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 6.993, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 4.922, 450, 250.0, 590.0, 0.0, 86.67], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 8.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 5.912, 420, 150.0, 640.0, 0.0, 83.33]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 2.934, 520, 200.0, 460.0, 0.0, 81.73], [13, 1, 11.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 6.793, 830, 270.0, 470.0, 0.0, 76.51], [16, 1, 5.841, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 13.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 4.864, 700, 160.0, 730.0, 0.0, 83.57], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 3.877, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 7.793, 890, 260.0, 750.0, 0.0, 83.15], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 15.55, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 9.534, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 13.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 5.792, 950, 230.0, 930.0, 0.0, 82.11], [37, 1, 9.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 5.852, 760, 190.0, 780.0, 0.0, 81.58], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 7.978, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 5.905, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 10.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 6.894, 420, 150.0, 640.0, 0.0, 84.52]], 'action': [32, 30, 37, 19, 35, 46, 13, 42, 43, 49, 27, 16, 38, 15, 23, 26, 29], 'reward': 4115.73468, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 2.934, 520, 200.0, 460.0, 0.0, 81.73], [13, 1, 11.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 6.793, 830, 270.0, 470.0, 0.0, 76.51], [16, 1, 5.841, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 13.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 4.864, 700, 160.0, 730.0, 0.0, 83.57], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 3.877, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 7.793, 890, 260.0, 750.0, 0.0, 83.15], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 15.55, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 9.534, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 13.0, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 5.792, 950, 230.0, 930.0, 0.0, 82.11], [37, 1, 9.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 5.852, 760, 190.0, 780.0, 0.0, 81.58], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 7.978, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 5.905, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 10.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 6.894, 420, 150.0, 640.0, 0.0, 84.52]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 3.919, 520, 200.0, 460.0, 0.0, 75.96], [13, 1, 13.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 7.775, 830, 270.0, 470.0, 0.0, 82.53], [16, 1, 6.824, 450, 150.0, 550.0, 0.0, 81.11], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 15.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 5.845, 700, 160.0, 730.0, 0.0, 86.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 3.877, 890, 230.0, 590.0, 0.0, 89.89], [27, 1, 8.767, 890, 260.0, 750.0, 0.0, 81.46], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 17.3, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 11.134, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 14.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 6.767, 950, 230.0, 930.0, 0.0, 82.63], [37, 1, 11.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 6.837, 760, 190.0, 780.0, 0.0, 76.32], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 8.966, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 6.886, 450, 250.0, 590.0, 0.0, 80.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 12.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 7.882, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [32, 30, 37, 19, 35, 46, 13, 42, 43, 49, 27, 36, 16, 38, 15, 12, 23], 'reward': 5093.05242, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 3.919, 520, 200.0, 460.0, 0.0, 75.96], [13, 1, 13.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 7.775, 830, 270.0, 470.0, 0.0, 82.53], [16, 1, 6.824, 450, 150.0, 550.0, 0.0, 81.11], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 15.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 5.845, 700, 160.0, 730.0, 0.0, 86.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 3.877, 890, 230.0, 590.0, 0.0, 89.89], [27, 1, 8.767, 890, 260.0, 750.0, 0.0, 81.46], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 17.3, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 11.134, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 14.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 6.767, 950, 230.0, 930.0, 0.0, 82.63], [37, 1, 11.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 6.837, 760, 190.0, 780.0, 0.0, 76.32], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 8.966, 310, 250.0, 430.0, 0.0, 85.48], [43, 1, 6.886, 450, 250.0, 590.0, 0.0, 80.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 1.978, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 12.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 7.882, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 0, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 0, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 4.896, 520, 200.0, 460.0, 0.0, 81.73], [13, 1, 15.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 8.756, 830, 270.0, 470.0, 0.0, 78.92], [16, 1, 7.808, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 17.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 6.819, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 4.843, 890, 230.0, 590.0, 0.0, 91.01], [27, 1, 9.738, 890, 260.0, 750.0, 0.0, 87.08], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 19.3, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 12.589, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 16.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 7.741, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 13.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 7.831, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 9.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 7.863, 450, 250.0, 590.0, 0.0, 90.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 14.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 7.882, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [32, 37, 30, 19, 35, 46, 13, 42, 43, 27, 36, 16, 38, 15, 12, 23, 26], 'reward': 5554.5010600000005, 'done': False}\n",
            "{'obs': [[0, 0, 0.0, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 0, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 4.896, 520, 200.0, 460.0, 0.0, 81.73], [13, 1, 15.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 8.756, 830, 270.0, 470.0, 0.0, 78.92], [16, 1, 7.808, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 17.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 6.819, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 4.843, 890, 230.0, 590.0, 0.0, 91.01], [27, 1, 9.738, 890, 260.0, 750.0, 0.0, 87.08], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 2.978, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 19.3, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 12.589, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 16.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 7.741, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 13.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 7.831, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 9.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 7.863, 450, 250.0, 590.0, 0.0, 90.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 1.978, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 14.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 7.882, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 5.873, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 17.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 9.737, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 8.789, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 19.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 7.813, 700, 160.0, 730.0, 0.0, 85.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 5.814, 890, 230.0, 590.0, 0.0, 87.08], [27, 0, 10.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 3.966, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 20.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 14.172, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 7.741, 950, 230.0, 930.0, 0.0, 85.26], [37, 1, 15.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 8.811, 760, 190.0, 780.0, 0.0, 79.61], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 9.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 8.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 2.962, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 16.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 86.49], [49, 1, 7.882, 420, 150.0, 640.0, 0.0, 85.71]], 'action': [32, 37, 30, 19, 35, 46, 13, 43, 27, 16, 38, 15, 12, 23, 26, 29, 45], 'reward': 5488.580569999999, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -18.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 5.873, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 17.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 9.737, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 8.789, 450, 150.0, 550.0, 0.0, 84.44], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 19.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 7.813, 700, 160.0, 730.0, 0.0, 85.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 5.814, 890, 230.0, 590.0, 0.0, 87.08], [27, 0, 10.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 3.966, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 20.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 14.172, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 7.741, 950, 230.0, 930.0, 0.0, 85.26], [37, 1, 15.0, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 8.811, 760, 190.0, 780.0, 0.0, 79.61], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 9.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 8.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 2.962, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 16.0, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 1.976, 740, 130.0, 930.0, 0.0, 86.49], [49, 1, 7.882, 420, 150.0, 640.0, 0.0, 85.71]], 'obs_': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 97.5], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 6.854, 520, 200.0, 460.0, 0.0, 79.81], [13, 1, 19.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 9.737, 830, 270.0, 470.0, 0.0, 78.31], [16, 1, 9.77, 450, 150.0, 550.0, 0.0, 85.56], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 19.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 8.799, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 6.783, 890, 230.0, 590.0, 0.0, 84.83], [27, 1, 10.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 4.957, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 22.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 15.772, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 8.719, 950, 230.0, 930.0, 0.0, 84.21], [37, 0, 16.8, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 9.796, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 11.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 10.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 3.958, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 17.5, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 2.955, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 8.864, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [32, 37, 30, 46, 13, 43, 42, 49, 16, 38, 36, 12, 23, 26, 29, 45, 48], 'reward': 5288.23499, 'done': False}\n",
            "{'obs': [[0, 1, 0.0, 320, 190.0, 260.0, 0.0, 97.5], [1, 1, -18.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 6.854, 520, 200.0, 460.0, 0.0, 79.81], [13, 1, 19.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 9.737, 830, 270.0, 470.0, 0.0, 78.31], [16, 1, 9.77, 450, 150.0, 550.0, 0.0, 85.56], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 19.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 8.799, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 6.783, 890, 230.0, 590.0, 0.0, 84.83], [27, 1, 10.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 4.957, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 22.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 15.772, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 8.719, 950, 230.0, 930.0, 0.0, 84.21], [37, 0, 16.8, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 9.796, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 11.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 10.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 3.958, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 17.5, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 2.955, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 8.864, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 7.84, 520, 200.0, 460.0, 0.0, 79.81], [13, 1, 21.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 10.715, 830, 270.0, 470.0, 0.0, 80.12], [16, 1, 10.753, 450, 150.0, 550.0, 0.0, 84.44], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 21.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 9.781, 700, 160.0, 730.0, 0.0, 86.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 7.761, 890, 230.0, 590.0, 0.0, 88.76], [27, 0, 12.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 4.957, 140, 140.0, 270.0, 0.0, 67.86], [30, 1, 24.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 17.216, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 9.699, 950, 230.0, 930.0, 0.0, 85.79], [37, 1, 16.8, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 10.779, 760, 190.0, 780.0, 0.0, 77.63], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 13.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 10.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 4.954, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 19.5, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 2.955, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 8.864, 420, 150.0, 640.0, 0.0, 86.9]], 'action': [32, 30, 19, 46, 13, 42, 27, 16, 38, 36, 15, 12, 23, 26, 45, 0, 1], 'reward': 5486.08548, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 7.84, 520, 200.0, 460.0, 0.0, 79.81], [13, 1, 21.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 10.715, 830, 270.0, 470.0, 0.0, 80.12], [16, 1, 10.753, 450, 150.0, 550.0, 0.0, 84.44], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 21.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 9.781, 700, 160.0, 730.0, 0.0, 86.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 7.761, 890, 230.0, 590.0, 0.0, 88.76], [27, 0, 12.712, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 4.957, 140, 140.0, 270.0, 0.0, 67.86], [30, 1, 24.967, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 17.216, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 18.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 9.699, 950, 230.0, 930.0, 0.0, 85.79], [37, 1, 16.8, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 10.779, 760, 190.0, 780.0, 0.0, 77.63], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 13.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 10.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 4.954, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 19.5, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 2.955, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 8.864, 420, 150.0, 640.0, 0.0, 86.9]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 8.826, 520, 200.0, 460.0, 0.0, 85.58], [13, 1, 23.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 11.701, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 11.739, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 23.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 10.769, 700, 160.0, 730.0, 0.0, 85.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 8.736, 890, 230.0, 590.0, 0.0, 88.76], [27, 1, 12.712, 890, 260.0, 750.0, 0.0, 90.45], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 5.95, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 26.467, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 17.216, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 20.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 10.686, 950, 230.0, 930.0, 0.0, 84.21], [37, 0, 18.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 11.764, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 13.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 12.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 5.944, 410, 80.0, 710.0, 0.0, 87.8], [46, 1, 21.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 2.955, 740, 130.0, 930.0, 0.0, 86.49], [49, 0, 9.853, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [30, 37, 19, 46, 13, 35, 43, 49, 16, 38, 36, 15, 12, 23, 26, 29, 45], 'reward': 7265.125169999999, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 8.826, 520, 200.0, 460.0, 0.0, 85.58], [13, 1, 23.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 11.701, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 11.739, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 23.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 10.769, 700, 160.0, 730.0, 0.0, 85.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 8.736, 890, 230.0, 590.0, 0.0, 88.76], [27, 1, 12.712, 890, 260.0, 750.0, 0.0, 90.45], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 5.95, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 26.467, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 17.216, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.0, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 20.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 10.686, 950, 230.0, 930.0, 0.0, 84.21], [37, 0, 18.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 11.764, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 13.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 12.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 5.944, 410, 80.0, 710.0, 0.0, 87.8], [46, 1, 21.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 0.0, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 2.955, 740, 130.0, 930.0, 0.0, 86.49], [49, 0, 9.853, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 9.809, 520, 200.0, 460.0, 0.0, 83.65], [13, 0, 25.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 11.701, 830, 270.0, 470.0, 0.0, 83.73], [16, 1, 12.722, 450, 150.0, 550.0, 0.0, 0.0], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 11.753, 700, 160.0, 730.0, 0.0, 87.14], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 9.714, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 13.689, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 5.95, 140, 140.0, 270.0, 0.0, 89.29], [30, 0, 27.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 19.073, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.995, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 22.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 11.668, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 18.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 12.747, 760, 190.0, 780.0, 0.0, 79.61], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 15.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 12.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 23.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 85.54], [48, 1, 3.936, 740, 130.0, 930.0, 0.0, 85.14], [49, 1, 9.853, 420, 150.0, 640.0, 0.0, 85.71]], 'action': [30, 32, 19, 46, 13, 35, 42, 27, 16, 38, 36, 12, 23, 26, 45, 48, 34], 'reward': 7306.21294, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -18.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 9.809, 520, 200.0, 460.0, 0.0, 83.65], [13, 0, 25.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 11.701, 830, 270.0, 470.0, 0.0, 83.73], [16, 1, 12.722, 450, 150.0, 550.0, 0.0, 0.0], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 11.753, 700, 160.0, 730.0, 0.0, 87.14], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 9.714, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 13.689, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 5.95, 140, 140.0, 270.0, 0.0, 89.29], [30, 0, 27.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 19.073, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 0.995, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 22.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 11.668, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 18.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 12.747, 760, 190.0, 780.0, 0.0, 79.61], [39, 0, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 15.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 12.842, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 23.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.0, 830, 210.0, 340.0, 0.0, 85.54], [48, 1, 3.936, 740, 130.0, 930.0, 0.0, 85.14], [49, 1, 9.853, 420, 150.0, 640.0, 0.0, 85.71]], 'obs_': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 10.8, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 25.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 12.684, 830, 270.0, 470.0, 0.0, 80.72], [16, 1, 14.722, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 12.738, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 9.714, 890, 230.0, 590.0, 0.0, 89.33], [27, 1, 15.689, 890, 260.0, 750.0, 0.0, 88.2], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 6.941, 140, 140.0, 270.0, 0.0, 75.0], [30, 1, 27.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 20.74, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 1.98, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 24.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 11.668, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 20.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 13.739, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 17.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 12.842, 450, 250.0, 590.0, 0.0, 91.11], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 25.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.98, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 10.836, 420, 150.0, 640.0, 0.0, 84.52]], 'action': [32, 37, 46, 35, 42, 27, 16, 49, 38, 15, 12, 23, 29, 48, 34, 7, 47], 'reward': 5742.975289999999, 'done': False}\n",
            "{'obs': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 10.8, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 25.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 12.684, 830, 270.0, 470.0, 0.0, 80.72], [16, 1, 14.722, 450, 150.0, 550.0, 0.0, 82.22], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 12.738, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 9.714, 890, 230.0, 590.0, 0.0, 89.33], [27, 1, 15.689, 890, 260.0, 750.0, 0.0, 88.2], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 6.941, 140, 140.0, 270.0, 0.0, 75.0], [30, 1, 27.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 20.74, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 1.98, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 24.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 11.668, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 20.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 13.739, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 17.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 12.842, 450, 250.0, 590.0, 0.0, 91.11], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 25.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.98, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 10.836, 420, 150.0, 640.0, 0.0, 84.52]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 11.785, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 27.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 13.671, 830, 270.0, 470.0, 0.0, 0.0], [16, 0, 15.706, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 12.738, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 10.691, 890, 230.0, 590.0, 0.0, 92.13], [27, 1, 16.669, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 0, 29.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 22.195, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 90.58], [34, 1, 1.98, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 26.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 12.651, 950, 230.0, 930.0, 0.0, 87.37], [37, 0, 22.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 14.729, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 92.41], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 19.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 13.827, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 27.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.98, 830, 210.0, 340.0, 0.0, 85.54], [48, 0, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 11.822, 420, 150.0, 640.0, 0.0, 84.52]], 'action': [32, 30, 37, 46, 35, 13, 42, 43, 27, 16, 49, 38, 36, 15, 12, 26, 29], 'reward': 10271.96515, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -9.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 11.785, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 27.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 13.671, 830, 270.0, 470.0, 0.0, 0.0], [16, 0, 15.706, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 25.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 0, 12.738, 700, 160.0, 730.0, 0.0, 0.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 10.691, 890, 230.0, 590.0, 0.0, 92.13], [27, 1, 16.669, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 0, 29.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 22.195, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.0, 690, 110.0, 750.0, 0.0, 90.58], [34, 1, 1.98, 560, 110.0, 530.0, 0.0, 85.71], [35, 1, 26.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 12.651, 950, 230.0, 930.0, 0.0, 87.37], [37, 0, 22.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 14.729, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.0, 790, 280.0, 280.0, 0.0, 92.41], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 19.949, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 13.827, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 27.167, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 0.98, 830, 210.0, 340.0, 0.0, 85.54], [48, 0, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 11.822, 420, 150.0, 640.0, 0.0, 84.52]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 12.769, 520, 200.0, 460.0, 0.0, 85.58], [13, 1, 29.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 13.671, 830, 270.0, 470.0, 0.0, 81.33], [16, 1, 15.706, 450, 150.0, 550.0, 0.0, 86.67], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 27.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 12.738, 700, 160.0, 730.0, 0.0, 91.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 11.669, 890, 230.0, 590.0, 0.0, 91.01], [27, 1, 18.669, 890, 260.0, 750.0, 0.0, 91.01], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 29.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 23.82, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.982, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 2.972, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 28.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 13.637, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 22.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 15.715, 760, 190.0, 780.0, 0.0, 83.55], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 20.949, 310, 250.0, 430.0, 0.0, 93.55], [43, 1, 13.827, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 28.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 1.961, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 12.805, 420, 150.0, 640.0, 0.0, 88.1]], 'action': [32, 46, 19, 35, 13, 42, 27, 49, 38, 36, 12, 26, 34, 33, 47, 39, 9], 'reward': 5677.7740699999995, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 12.769, 520, 200.0, 460.0, 0.0, 85.58], [13, 1, 29.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 13.671, 830, 270.0, 470.0, 0.0, 81.33], [16, 1, 15.706, 450, 150.0, 550.0, 0.0, 86.67], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 27.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 12.738, 700, 160.0, 730.0, 0.0, 91.43], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 11.669, 890, 230.0, 590.0, 0.0, 91.01], [27, 1, 18.669, 890, 260.0, 750.0, 0.0, 91.01], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 29.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 0, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 23.82, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.982, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 2.972, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 28.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 13.637, 950, 230.0, 930.0, 0.0, 0.0], [37, 1, 22.181, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 15.715, 760, 190.0, 780.0, 0.0, 83.55], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 20.949, 310, 250.0, 430.0, 0.0, 93.55], [43, 1, 13.827, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 6.929, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 28.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 1.961, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 4.917, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 12.805, 420, 150.0, 640.0, 0.0, 88.1]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 0, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 13.757, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 31.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 14.656, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 16.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 29.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 13.734, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 12.644, 890, 230.0, 590.0, 0.0, 87.08], [27, 1, 19.65, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 31.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 25.42, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.982, 690, 110.0, 750.0, 0.0, 89.86], [34, 1, 2.972, 560, 110.0, 530.0, 0.0, 86.61], [35, 1, 30.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 13.637, 950, 230.0, 930.0, 0.0, 85.79], [37, 1, 23.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 16.701, 760, 190.0, 780.0, 0.0, 78.95], [39, 0, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 21.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 14.81, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 6.929, 410, 80.0, 710.0, 0.0, 87.8], [46, 0, 30.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 1.961, 830, 210.0, 340.0, 0.0, 86.75], [48, 1, 4.917, 740, 130.0, 930.0, 0.0, 90.54], [49, 1, 13.79, 420, 150.0, 640.0, 0.0, 85.71]], 'action': [32, 46, 30, 19, 37, 35, 13, 42, 27, 43, 16, 49, 38, 12, 15, 23, 26], 'reward': 13250.84677, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 0, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 13.757, 520, 200.0, 460.0, 0.0, 0.0], [13, 1, 31.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 14.656, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 16.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 0, 29.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 13.734, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 12.644, 890, 230.0, 590.0, 0.0, 87.08], [27, 1, 19.65, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 7.939, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 31.753, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 25.42, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 0.982, 690, 110.0, 750.0, 0.0, 89.86], [34, 1, 2.972, 560, 110.0, 530.0, 0.0, 86.61], [35, 1, 30.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 13.637, 950, 230.0, 930.0, 0.0, 85.79], [37, 1, 23.514, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 16.701, 760, 190.0, 780.0, 0.0, 78.95], [39, 0, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 21.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 14.81, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 6.929, 410, 80.0, 710.0, 0.0, 87.8], [46, 0, 30.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 1.961, 830, 210.0, 340.0, 0.0, 86.75], [48, 1, 4.917, 740, 130.0, 930.0, 0.0, 90.54], [49, 1, 13.79, 420, 150.0, 640.0, 0.0, 85.71]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 13.757, 520, 200.0, 460.0, 0.0, 80.77], [13, 0, 33.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 14.656, 830, 270.0, 470.0, 0.0, 0.0], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 29.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 14.719, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 13.621, 890, 230.0, 590.0, 0.0, 89.89], [27, 1, 21.65, 890, 260.0, 750.0, 0.0, 88.2], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 7.939, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 33.42, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 25.42, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 3.962, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 32.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 14.619, 950, 230.0, 930.0, 0.0, 84.21], [37, 1, 25.114, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 17.686, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 23.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 14.81, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 7.926, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 30.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 5.9, 740, 130.0, 930.0, 0.0, 90.54], [49, 1, 14.776, 420, 150.0, 640.0, 0.0, 83.33]], 'action': [30, 37, 35, 13, 42, 27, 16, 49, 38, 36, 23, 26, 45, 48, 34, 33, 47], 'reward': 7743.057030000001, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 13.757, 520, 200.0, 460.0, 0.0, 80.77], [13, 0, 33.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 14.656, 830, 270.0, 470.0, 0.0, 0.0], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 29.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 14.719, 700, 160.0, 730.0, 0.0, 89.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 13.621, 890, 230.0, 590.0, 0.0, 89.89], [27, 1, 21.65, 890, 260.0, 750.0, 0.0, 88.2], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 7.939, 140, 140.0, 270.0, 0.0, 78.57], [30, 1, 33.42, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 25.42, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 3.962, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 32.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 14.619, 950, 230.0, 930.0, 0.0, 84.21], [37, 1, 25.114, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 17.686, 760, 190.0, 780.0, 0.0, 77.63], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 23.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 14.81, 450, 250.0, 590.0, 0.0, 0.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 7.926, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 30.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 5.9, 740, 130.0, 930.0, 0.0, 90.54], [49, 1, 14.776, 420, 150.0, 640.0, 0.0, 83.33]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 0, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 14.744, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 33.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 14.656, 830, 270.0, 470.0, 0.0, 81.93], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 31.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 15.71, 700, 160.0, 730.0, 0.0, 90.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 14.6, 890, 230.0, 590.0, 0.0, 92.13], [27, 0, 22.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 8.937, 140, 140.0, 270.0, 0.0, 82.14], [30, 0, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 26.864, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 3.962, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 34.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 15.608, 950, 230.0, 930.0, 0.0, 87.37], [37, 1, 27.114, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 18.673, 760, 190.0, 780.0, 0.0, 78.95], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 25.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 14.81, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 8.914, 410, 80.0, 710.0, 0.0, 90.24], [46, 1, 32.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 6.886, 740, 130.0, 930.0, 0.0, 88.51], [49, 0, 15.762, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [32, 30, 46, 35, 37, 19, 42, 27, 49, 38, 36, 12, 23, 26, 29, 45, 48], 'reward': 11459.19609, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 0, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 14.744, 520, 200.0, 460.0, 0.0, 83.65], [13, 1, 33.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 14.656, 830, 270.0, 470.0, 0.0, 81.93], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 31.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 15.71, 700, 160.0, 730.0, 0.0, 90.71], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 14.6, 890, 230.0, 590.0, 0.0, 92.13], [27, 0, 22.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 8.937, 140, 140.0, 270.0, 0.0, 82.14], [30, 0, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 26.864, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 3.962, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 34.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 15.608, 950, 230.0, 930.0, 0.0, 87.37], [37, 1, 27.114, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 18.673, 760, 190.0, 780.0, 0.0, 78.95], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 25.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 14.81, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 8.914, 410, 80.0, 710.0, 0.0, 90.24], [46, 1, 32.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 6.886, 740, 130.0, 930.0, 0.0, 88.51], [49, 0, 15.762, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 15.732, 520, 200.0, 460.0, 0.0, 87.5], [13, 1, 35.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 15.641, 830, 270.0, 470.0, 0.0, 84.34], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 33.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 16.698, 700, 160.0, 730.0, 0.0, 92.86], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 15.578, 890, 230.0, 590.0, 0.0, 90.45], [27, 1, 22.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 9.935, 140, 140.0, 270.0, 0.0, 82.14], [30, 0, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 28.409, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 3.962, 560, 110.0, 530.0, 0.0, 87.5], [35, 1, 36.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 16.598, 950, 230.0, 930.0, 0.0, 87.37], [37, 1, 28.914, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 19.661, 760, 190.0, 780.0, 0.0, 0.0], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 27.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 15.795, 450, 250.0, 590.0, 0.0, 87.78], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 9.903, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 34.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 15.762, 420, 150.0, 640.0, 0.0, 86.9]], 'action': [32, 46, 35, 37, 19, 13, 42, 43, 38, 36, 12, 23, 15, 26, 29, 45, 48], 'reward': 11620.796690000001, 'done': False}\n",
            "{'obs': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 15.732, 520, 200.0, 460.0, 0.0, 87.5], [13, 1, 35.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 0, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 15.641, 830, 270.0, 470.0, 0.0, 84.34], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 33.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 16.698, 700, 160.0, 730.0, 0.0, 92.86], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 15.578, 890, 230.0, 590.0, 0.0, 90.45], [27, 1, 22.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 9.935, 140, 140.0, 270.0, 0.0, 82.14], [30, 0, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 0, 28.409, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 3.962, 560, 110.0, 530.0, 0.0, 87.5], [35, 1, 36.5, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 16.598, 950, 230.0, 930.0, 0.0, 87.37], [37, 1, 28.914, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 19.661, 760, 190.0, 780.0, 0.0, 0.0], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 27.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 15.795, 450, 250.0, 590.0, 0.0, 87.78], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 9.903, 410, 80.0, 710.0, 0.0, 89.02], [46, 1, 34.967, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 15.762, 420, 150.0, 640.0, 0.0, 86.9]], 'obs_': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 16.721, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 37.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 16.628, 830, 270.0, 470.0, 0.0, 81.33], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 35.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 17.686, 700, 160.0, 730.0, 0.0, 88.57], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 16.563, 890, 230.0, 590.0, 0.0, 92.7], [27, 1, 24.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 10.93, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 28.409, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 4.953, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 17.584, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 30.485, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 19.661, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 29.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 16.785, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 10.895, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 36.681, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 16.75, 420, 150.0, 640.0, 0.0, 84.52]], 'action': [46, 35, 37, 19, 13, 42, 27, 43, 49, 36, 12, 23, 15, 26, 29, 45, 34], 'reward': 12126.55444, 'done': False}\n",
            "{'obs': [[0, 0, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 16.721, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 37.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 16.628, 830, 270.0, 470.0, 0.0, 81.33], [16, 0, 18.689, 450, 150.0, 550.0, 0.0, 0.0], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 35.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 17.686, 700, 160.0, 730.0, 0.0, 88.57], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 16.563, 890, 230.0, 590.0, 0.0, 92.7], [27, 1, 24.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 10.93, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 35.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 28.409, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 4.953, 560, 110.0, 530.0, 0.0, 0.0], [35, 0, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 17.584, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 30.485, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 19.661, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 0, 29.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 16.785, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 10.895, 410, 80.0, 710.0, 0.0, 86.59], [46, 1, 36.681, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 16.75, 420, 150.0, 640.0, 0.0, 84.52]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 17.708, 520, 200.0, 460.0, 0.0, 84.62], [13, 1, 39.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 17.614, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 18.689, 450, 150.0, 550.0, 0.0, 87.78], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 37.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 18.683, 700, 160.0, 730.0, 0.0, 88.57], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 17.543, 890, 230.0, 590.0, 0.0, 89.33], [27, 1, 26.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 11.924, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 37.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 30.266, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 91.3], [34, 1, 4.953, 560, 110.0, 530.0, 0.0, 85.71], [35, 0, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 18.57, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 32.152, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 20.647, 760, 190.0, 780.0, 0.0, 0.0], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 29.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 17.768, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 11.883, 410, 80.0, 710.0, 0.0, 89.02], [46, 0, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 17.735, 420, 150.0, 640.0, 0.0, 83.33]], 'action': [46, 37, 32, 19, 30, 13, 27, 43, 49, 38, 36, 12, 23, 15, 26, 29, 45], 'reward': 13861.558920000001, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 1.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 17.708, 520, 200.0, 460.0, 0.0, 84.62], [13, 1, 39.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 17.614, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 18.689, 450, 150.0, 550.0, 0.0, 87.78], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 37.667, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 18.683, 700, 160.0, 730.0, 0.0, 88.57], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 17.543, 890, 230.0, 590.0, 0.0, 89.33], [27, 1, 26.633, 890, 260.0, 750.0, 0.0, 0.0], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 11.924, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 37.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 30.266, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 1.964, 690, 110.0, 750.0, 0.0, 91.3], [34, 1, 4.953, 560, 110.0, 530.0, 0.0, 85.71], [35, 0, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 18.57, 950, 230.0, 930.0, 0.0, 86.32], [37, 1, 32.152, 170, 180.0, 960.0, 0.0, 0.0], [38, 0, 20.647, 760, 190.0, 780.0, 0.0, 0.0], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 29.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 17.768, 450, 250.0, 590.0, 0.0, 82.22], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 11.883, 410, 80.0, 710.0, 0.0, 89.02], [46, 0, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 0, 7.875, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 17.735, 420, 150.0, 640.0, 0.0, 83.33]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 18.693, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 41.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 17.614, 830, 270.0, 470.0, 0.0, 86.75], [16, 0, 19.673, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 39.096, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 19.669, 700, 160.0, 730.0, 0.0, 87.86], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 18.527, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 28.633, 890, 260.0, 750.0, 0.0, 92.13], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 11.924, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 39.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 32.123, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 2.96, 690, 110.0, 750.0, 0.0, 89.13], [34, 0, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 19.555, 950, 230.0, 930.0, 0.0, 87.89], [37, 1, 33.952, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 20.647, 760, 190.0, 780.0, 0.0, 82.24], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 31.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 18.756, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 12.87, 410, 80.0, 710.0, 0.0, 90.24], [46, 0, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 7.875, 740, 130.0, 930.0, 0.0, 91.89], [49, 0, 18.72, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [19, 32, 37, 30, 13, 42, 27, 43, 49, 36, 16, 12, 23, 26, 45, 34, 33], 'reward': 12797.97728, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 0, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 18.693, 520, 200.0, 460.0, 0.0, 86.54], [13, 1, 41.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 17.614, 830, 270.0, 470.0, 0.0, 86.75], [16, 0, 19.673, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 39.096, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 19.669, 700, 160.0, 730.0, 0.0, 87.86], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 0, 18.527, 890, 230.0, 590.0, 0.0, 0.0], [27, 1, 28.633, 890, 260.0, 750.0, 0.0, 92.13], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 11.924, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 39.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 32.123, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 2.96, 690, 110.0, 750.0, 0.0, 89.13], [34, 0, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 38.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 19.555, 950, 230.0, 930.0, 0.0, 87.89], [37, 1, 33.952, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 20.647, 760, 190.0, 780.0, 0.0, 82.24], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 0, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 0, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 31.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 0, 18.756, 450, 250.0, 590.0, 0.0, 0.0], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 12.87, 410, 80.0, 710.0, 0.0, 90.24], [46, 0, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 0, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 7.875, 740, 130.0, 930.0, 0.0, 91.89], [49, 0, 18.72, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 19.678, 520, 200.0, 460.0, 0.0, 84.62], [13, 0, 43.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 18.598, 830, 270.0, 470.0, 0.0, 83.13], [16, 1, 19.673, 450, 150.0, 550.0, 0.0, 88.89], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 41.096, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 20.656, 700, 160.0, 730.0, 0.0, 92.86], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 18.527, 890, 230.0, 590.0, 0.0, 90.45], [27, 1, 29.615, 890, 260.0, 750.0, 0.0, 90.45], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 12.916, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 41.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 33.623, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 40.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 20.54, 950, 230.0, 930.0, 0.0, 90.0], [37, 1, 35.785, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 21.635, 760, 190.0, 780.0, 0.0, 80.92], [39, 0, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 33.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 18.756, 450, 250.0, 590.0, 0.0, 90.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 13.858, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 18.72, 420, 150.0, 640.0, 0.0, 0.0]], 'action': [32, 37, 19, 30, 13, 35, 42, 27, 38, 36, 12, 23, 15, 29, 45, 48, 33], 'reward': 14313.24129, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 0, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 0.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 19.678, 520, 200.0, 460.0, 0.0, 84.62], [13, 0, 43.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 18.598, 830, 270.0, 470.0, 0.0, 83.13], [16, 1, 19.673, 450, 150.0, 550.0, 0.0, 88.89], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 41.096, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 20.656, 700, 160.0, 730.0, 0.0, 92.86], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 0, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 18.527, 890, 230.0, 590.0, 0.0, 90.45], [27, 1, 29.615, 890, 260.0, 750.0, 0.0, 90.45], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 12.916, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 41.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 33.623, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 0, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 40.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 20.54, 950, 230.0, 930.0, 0.0, 90.0], [37, 1, 35.785, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 21.635, 760, 190.0, 780.0, 0.0, 80.92], [39, 0, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 33.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 18.756, 450, 250.0, 590.0, 0.0, 90.0], [44, 0, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 13.858, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 38.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 0, 18.72, 420, 150.0, 640.0, 0.0, 0.0]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 20.666, 520, 200.0, 460.0, 0.0, 88.46], [13, 1, 43.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 19.594, 830, 270.0, 470.0, 0.0, 85.54], [16, 1, 20.658, 450, 150.0, 550.0, 0.0, 86.67], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 42.846, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 21.643, 700, 160.0, 730.0, 0.0, 90.0], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 19.509, 890, 230.0, 590.0, 0.0, 91.57], [27, 1, 30.601, 890, 260.0, 750.0, 0.0, 89.33], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 13.908, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 43.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 35.623, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 42.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 21.526, 950, 230.0, 930.0, 0.0, 0.0], [37, 0, 37.229, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 22.622, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 35.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 19.742, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 13.858, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 40.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 18.72, 420, 150.0, 640.0, 0.0, 86.9]], 'action': [37, 19, 32, 30, 35, 46, 42, 27, 43, 38, 36, 16, 12, 23, 15, 26, 29], 'reward': 17844.80057, 'done': False}\n",
            "{'obs': [[0, 1, -39.075, 320, 190.0, 260.0, 1.0, 0.0], [1, 0, -28.2, 360, 290.0, 770.0, 1.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 1.0, 0.0], [7, 1, -28.0, 160, 120.0, 670.0, 1.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 1.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 20.666, 520, 200.0, 460.0, 0.0, 88.46], [13, 1, 43.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 19.594, 830, 270.0, 470.0, 0.0, 85.54], [16, 1, 20.658, 450, 150.0, 550.0, 0.0, 86.67], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 0, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 42.846, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 1, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 21.643, 700, 160.0, 730.0, 0.0, 90.0], [24, 0, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 19.509, 890, 230.0, 590.0, 0.0, 91.57], [27, 1, 30.601, 890, 260.0, 750.0, 0.0, 89.33], [28, 0, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 13.908, 140, 140.0, 270.0, 0.0, 82.14], [30, 1, 43.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 35.623, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 42.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 0, 21.526, 950, 230.0, 930.0, 0.0, 0.0], [37, 0, 37.229, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 22.622, 760, 190.0, 780.0, 0.0, 80.92], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 35.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 19.742, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 13.858, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 40.181, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 18.72, 420, 150.0, 640.0, 0.0, 86.9]], 'obs_': [[0, 0, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 21.654, 520, 200.0, 460.0, 0.0, 0.0], [13, 0, 45.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 20.585, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 21.647, 450, 150.0, 550.0, 0.0, 85.56], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 44.846, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 22.632, 700, 160.0, 730.0, 0.0, 95.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 20.493, 890, 230.0, 590.0, 0.0, 93.26], [27, 1, 31.587, 890, 260.0, 750.0, 0.0, 91.57], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 14.906, 140, 140.0, 270.0, 0.0, 85.71], [30, 1, 45.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 37.168, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 44.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 21.526, 950, 230.0, 930.0, 0.0, 88.95], [37, 1, 37.229, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 23.611, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 37.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 20.729, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 13.858, 410, 80.0, 710.0, 0.0, 91.46], [46, 1, 41.681, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 19.711, 420, 150.0, 640.0, 0.0, 83.33]], 'action': [32, 19, 30, 35, 46, 13, 42, 27, 43, 38, 49, 16, 12, 23, 15, 26, 29], 'reward': 18313.222279999998, 'done': False}\n",
            "{'obs': [[0, 0, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 1.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 1, -31.054, 800, 280.0, 640.0, 0.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 0.0, 0.0], [6, 1, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 1, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 1, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 1, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 1, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 0, 21.654, 520, 200.0, 460.0, 0.0, 0.0], [13, 0, 45.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 0, 20.585, 830, 270.0, 470.0, 0.0, 0.0], [16, 1, 21.647, 450, 150.0, 550.0, 0.0, 85.56], [17, 0, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 44.846, 550, 240.0, 970.0, 0.0, 0.0], [20, 0, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 0, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 22.632, 700, 160.0, 730.0, 0.0, 95.0], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 20.493, 890, 230.0, 590.0, 0.0, 93.26], [27, 1, 31.587, 890, 260.0, 750.0, 0.0, 91.57], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 1, 14.906, 140, 140.0, 270.0, 0.0, 85.71], [30, 1, 45.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 37.168, 100, 290.0, 910.0, 0.0, 0.0], [33, 0, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 44.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 21.526, 950, 230.0, 930.0, 0.0, 88.95], [37, 1, 37.229, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 23.611, 760, 190.0, 780.0, 0.0, 78.29], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 37.934, 310, 250.0, 430.0, 0.0, 0.0], [43, 1, 20.729, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 1, 13.858, 410, 80.0, 710.0, 0.0, 91.46], [46, 1, 41.681, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 19.711, 420, 150.0, 640.0, 0.0, 83.33]], 'obs_': [[0, 1, -39.075, 320, 190.0, 260.0, 0.0, 0.0], [1, 1, -28.2, 360, 290.0, 770.0, 0.0, 0.0], [2, 1, -45.39, 510, 100.0, 730.0, 0.0, 0.0], [3, 1, -28.219, 450, 180.0, 450.0, 0.0, 0.0], [4, 0, -31.054, 800, 280.0, 640.0, 1.0, 0.0], [5, 1, -10.0, 130, 190.0, 270.0, 1.0, 0.0], [6, 0, -54.207, 760, 240.0, 830.0, 0.0, 0.0], [7, 0, -28.0, 160, 120.0, 670.0, 0.0, 0.0], [8, 0, -26.812, 380, 260.0, 540.0, 1.0, 0.0], [9, 0, -19.019, 300, 60.0, 990.0, 0.0, 0.0], [10, 0, 0.0, 610, 50.0, 700.0, 0.0, 0.0], [11, 0, 0.0, 410, 250.0, 180.0, 0.0, 0.0], [12, 1, 21.654, 520, 200.0, 460.0, 0.0, 90.38], [13, 1, 45.95, 840, 300.0, 750.0, 0.0, 0.0], [14, 1, -20.317, 700, 180.0, 890.0, 0.0, 0.0], [15, 1, 20.585, 830, 270.0, 470.0, 0.0, 81.33], [16, 0, 22.634, 450, 150.0, 550.0, 0.0, 0.0], [17, 1, 0.0, 460, 70.0, 360.0, 0.0, 0.0], [18, 1, 0.0, 650, 160.0, 240.0, 0.0, 0.0], [19, 1, 46.275, 550, 240.0, 970.0, 0.0, 0.0], [20, 1, 0.0, 210, 70.0, 160.0, 0.0, 0.0], [21, 0, 0.0, 580, 290.0, 180.0, 0.0, 0.0], [22, 1, 0.0, 500, 220.0, 190.0, 0.0, 0.0], [23, 1, 23.629, 700, 160.0, 730.0, 0.0, 94.29], [24, 1, 0.0, 1000, 110.0, 430.0, 0.0, 0.0], [25, 1, 0.0, 850, 240.0, 160.0, 0.0, 0.0], [26, 1, 21.477, 890, 230.0, 590.0, 0.0, 92.7], [27, 1, 32.572, 890, 260.0, 750.0, 0.0, 90.45], [28, 1, 0.0, 630, 60.0, 760.0, 0.0, 0.0], [29, 0, 15.9, 140, 140.0, 270.0, 0.0, 0.0], [30, 1, 47.17, 640, 260.0, 920.0, 0.0, 0.0], [31, 1, 0.0, 990, 90.0, 220.0, 0.0, 0.0], [32, 1, 39.168, 100, 290.0, 910.0, 0.0, 0.0], [33, 1, 3.957, 690, 110.0, 750.0, 0.0, 0.0], [34, 1, 5.939, 560, 110.0, 530.0, 0.0, 0.0], [35, 1, 40.25, 440, 190.0, 760.0, 0.0, 0.0], [36, 1, 22.512, 950, 230.0, 930.0, 0.0, 87.37], [37, 1, 38.829, 170, 180.0, 960.0, 0.0, 0.0], [38, 1, 24.601, 760, 190.0, 780.0, 0.0, 84.21], [39, 1, 0.99, 790, 280.0, 280.0, 0.0, 0.0], [40, 1, 0.0, 390, 110.0, 250.0, 0.0, 0.0], [41, 1, 0.0, 870, 100.0, 680.0, 0.0, 0.0], [42, 1, 33.934, 310, 250.0, 430.0, 0.0, 90.32], [43, 1, 21.716, 450, 250.0, 590.0, 0.0, 84.44], [44, 1, 0.0, 880, 120.0, 710.0, 0.0, 0.0], [45, 0, 14.849, 410, 80.0, 710.0, 0.0, 0.0], [46, 1, 43.681, 330, 210.0, 680.0, 0.0, 0.0], [47, 1, 2.945, 830, 210.0, 340.0, 0.0, 0.0], [48, 1, 8.864, 740, 130.0, 930.0, 0.0, 0.0], [49, 1, 20.698, 420, 150.0, 640.0, 0.0, 85.71]], 'action': [19, 32, 30, 37, 35, 46, 42, 27, 43, 38, 49, 36, 16, 23, 26, 29, 45], 'reward': 18795.19731, 'done': False}\n"
          ]
        }
      ],
      "source": [
        "round_reward = []\n",
        "transitions = []\n",
        "\n",
        "for block_id in range(len(node_info_by_block) - 1):  # Fixed the range\n",
        "    # print(len(node_info_by_block) - 1)\n",
        "    # print(f\"Block {block_id}:\")\n",
        "    nodes_rewards = []\n",
        "\n",
        "    for node_index in selected_nodes[block_id]:\n",
        "        # print(\"Node index: \", node_index)\n",
        "        node_data = nodes_by_block[block_id][node_index]\n",
        "        reward = node_data[-1]\n",
        "        # print(\"Original reward: \", reward)\n",
        "\n",
        "        if reward != 0.0:\n",
        "            reward *= node_data[2]\n",
        "        else :\n",
        "            reward = node_data[2]\n",
        "        # print(\"Modified reward: \", reward)\n",
        "\n",
        "        nodes_rewards.append(reward)\n",
        "\n",
        "    # print(\"Nodes rewards:\", nodes_rewards)\n",
        "    cumulated_reward = sum(nodes_rewards)\n",
        "    round_reward.append(cumulated_reward)\n",
        "\n",
        "    transition = {\n",
        "        \"obs\": nodes_by_block[block_id],\n",
        "        \"obs_\": nodes_by_block[block_id + 1],\n",
        "        \"action\": selected_nodes[block_id],\n",
        "        \"reward\": cumulated_reward,\n",
        "        \"done\": block_id == len(node_info_by_block) - 1  # Corrected the condition\n",
        "    }\n",
        "    transitions.append(transition)\n",
        "\n",
        "for transition in transitions :\n",
        "    print(transition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTWBCxysyg_O"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1ULASAi7JUoo"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    #beta learning rate, number of input dimensions from the environment\n",
        "    #\n",
        "    def __init__ (self,beta,input_shape,n_actions,\n",
        "    fc1_dims=256 , fc2_dims = 256, name='critic',chkpt_dir='tmp/sac' ):\n",
        "        super(CriticNetwork,self).__init__()\n",
        "        self.input_dims = np.prod(input_shape)\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,self.name+'_sac')\n",
        "        #layer 1\n",
        "        self.fc1 = nn.Linear(self.input_dims+n_actions,self.fc1_dims)\n",
        "        #layer 2\n",
        "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
        "        #layer 3\n",
        "        self.q = nn.Linear(self.fc2_dims,1)\n",
        "        #optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=beta)\n",
        "        #device\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        #to device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self,state,action):\n",
        "        #layer 1\n",
        "        input = T.cat([state,action],dim=0)\n",
        "        action_value = self.fc1(input)\n",
        "        action_value = F.relu(action_value)\n",
        "        #layer 2\n",
        "        action_value = self.fc2(action_value)\n",
        "        action_value = F.relu(action_value)\n",
        "        #layer 3\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        return q\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('...saving checkpoint...')\n",
        "        T.save(self.state_dict(),self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('...loading checkpoint...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self,beta,input_shape,fc1_dims=256,fc2_dims=256,name='value',chkpt_dir='tmp/sac'):\n",
        "        super(ValueNetwork,self).__init__()\n",
        "        # print(\"in init value networks\")\n",
        "        self.input_dims = np.prod(input_shape)\n",
        "\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,self.name+'_sac')\n",
        "        #layer 1\n",
        "        # print(\"value network input dimensions\", self.input_dims)\n",
        "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
        "        #layer 2\n",
        "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
        "        #layer 3\n",
        "        self.v = nn.Linear(self.fc2_dims,1)\n",
        "        #optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=beta)\n",
        "        #device\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        #to device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self,state):\n",
        "        #layer 1\n",
        "        state_value = self.fc1(state)\n",
        "        state_value = F.relu(state_value)\n",
        "        #layer 2\n",
        "        state_value = self.fc2(state_value)\n",
        "        state_value = F.relu(state_value)\n",
        "        #layer 3\n",
        "        v = self.v(state_value)\n",
        "\n",
        "        return v\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('...saving checkpoint...')\n",
        "        T.save(self.state_dict(),self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('...loading checkpoint...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self,alpha,input_shape,max_actions,fc1_dims=256,fc2_dims=256,n_actions=2,name='actor',chkpt_dir='tmp/sac'):\n",
        "        super(ActorNetwork,self).__init__()\n",
        "        self.input_dims = np.prod(input_shape)\n",
        "        self.input_shape = input_shape\n",
        "        self.max_actions = max_actions # number of selected nodes\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,self.name+'_sac')\n",
        "        # print(\"checpoint file\", self.checkpoint_file)\n",
        "        self.reparam_noise = 1e-6\n",
        "        #layer 1\n",
        "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
        "        # self.fc1 = nn.Linear(*self.input_dims,out_features=self.fc1_dims)\n",
        "        #layer 2\n",
        "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
        "        #layer 3\n",
        "\n",
        "\n",
        "        self.mu = nn.Linear(self.fc2_dims,self.max_actions)\n",
        "        #layer 4\n",
        "        self.sigma = nn.Linear(self.fc2_dims,self.max_actions)\n",
        "        #optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=alpha)\n",
        "        #device\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        #to device\n",
        "        self.to(self.device)\n",
        "        #normal distribution\n",
        "        # self.distribution = Normal\n",
        "\n",
        "    def sample_normal(self, state, num_selected_nodes, exploration_noise=0.025):\n",
        "        # print(\"in sample_normal\")\n",
        "        state = dr.flatten_nodes(state)\n",
        "        # print(\"numselected in sample normal\", num_selected_nodes)\n",
        "        action_probs, action_mean, action_log_std = self.forward(state)\n",
        "        # print(\"action probs in sample normazle\",action_probs.shape)\n",
        "        action_std = action_log_std.exp()\n",
        "        state = dr.array_to_state(state, 8)\n",
        "        # print(\"after array of array \",state)\n",
        "        availability_mask = state[:, 1] != 0\n",
        "        availability_mask = availability_mask.long()\n",
        "        # print(\"availability mask\", availability_mask)\n",
        "        # Add exploration noise to logits\n",
        "        noisy_logits = action_mean + exploration_noise * action_std * T.randn_like(action_mean)\n",
        "        action_probs_clean = T.tensor(np.nan_to_num(action_probs.T.detach().numpy(), nan=0.0))\n",
        "        # Create a Categorical distribution using the noisy logits\n",
        "\n",
        "        action_dist = Categorical(action_probs_clean)\n",
        "        # print(\"action_dist from categorical \", action_dist)\n",
        "        # Sample actions from the Categorical distribution\n",
        "        sampled_actions = action_dist.sample()\n",
        "        print('sampled actions from categorical', sampled_actions)\n",
        "\n",
        "        # Apply availability mask\n",
        "        # print('sampled actions size', sampled_actions)\n",
        "        # print('availability mask shape', availability_mask)\n",
        "        # sampled_actions = sampled_actions * availability_mask\n",
        "\n",
        "\n",
        "        # Get indices of selected nodes\n",
        "        selected_indices = sampled_actions.nonzero().squeeze()\n",
        "        for i in range(10):\n",
        "            new_samples = action_dist.sample()\n",
        "            new_samples = new_samples*availability_mask  # Apply availability mask\n",
        "            new_indices = new_samples.nonzero().squeeze()\n",
        "\n",
        "            # Convert the selected_indices list back to a tensor\n",
        "            selected_indices_tensor = T.tensor(selected_indices)\n",
        "\n",
        "            # Combine the selected indices and new indices while removing duplicates\n",
        "            combined_indices = T.cat((selected_indices_tensor, new_indices))\n",
        "            unique_combined_indices = T.unique(combined_indices)\n",
        "\n",
        "            # Convert the unique indices back to a Python list\n",
        "            selected_indices = unique_combined_indices\n",
        "        # print(selected_indices)\n",
        "        selected_indices = np.array(selected_indices)\n",
        "        # print(selected_indices)\n",
        "        if len(selected_indices) < num_selected_nodes:\n",
        "            additional_indices = np.random.choice(availability_mask.nonzero().squeeze(), size=num_selected_nodes - len(selected_indices), replace=False)\n",
        "            print(additional_indices)\n",
        "            selected_indices = np.concatenate((selected_indices, additional_indices))\n",
        "            # print(\"finished sample_normal\")\n",
        "        # Calculate log probabilities for the new sampled actions\n",
        "        log_probs = action_dist.log_prob(new_samples)\n",
        "        # print(\"log probs\", log_probs)\n",
        "        return selected_indices, log_probs\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "      # Layer 1\n",
        "      prob = self.fc1(state)\n",
        "      prob = F.relu(prob)\n",
        "      prob = self.fc2(prob)\n",
        "      prob = F.relu(prob)\n",
        "\n",
        "      # Layer 3\n",
        "      mu = self.mu(prob)\n",
        "      sigma = self.sigma(prob)\n",
        "\n",
        "      # Apply availability mask\n",
        "      state = dr.array_to_state(state, 8)\n",
        "      availability_mask = state[:, 1] != 0\n",
        "      availability_mask = availability_mask.float()\n",
        "\n",
        "      # Apply ReLU activation to mu to ensure non-negative values\n",
        "      mu = F.relu(mu)\n",
        "\n",
        "      # Scale the mu values to the range [0, 1]\n",
        "      scaled_actions = mu / self.input_shape[0]\n",
        "\n",
        "      # Calculate the probability of not selecting the node\n",
        "      prob_not_selected = 1 - scaled_actions\n",
        "\n",
        "      # Normalize the probabilities\n",
        "      total_probabilities = prob_not_selected + scaled_actions\n",
        "      selection_probabilities = scaled_actions / total_probabilities  # Only the probability of selecting the node\n",
        "\n",
        "      return selection_probabilities, mu, sigma\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('...saving checkpoint...')\n",
        "        T.save(self.state_dict(),self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('...loading checkpoint...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "X5YgPIKeJU1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "ALPHA_INITIAL = 1.\n",
        "DISCOUNT_RATE = 0.99\n",
        "LEARNING_RATE = 10 ** -4\n",
        "SOFT_UPDATE_INTERPOLATION_FACTOR = 0.01\n",
        "class Agent ():\n",
        "    def __init__(self,env,alpha=ALPHA_INITIAL,beta=LEARNING_RATE,input_shape=[8],gamma = DISCOUNT_RATE,n_actions=2,max_actions=1,max_size=200,tau=SOFT_UPDATE_INTERPOLATION_FACTOR,\n",
        "    layer1_size=256,layer2_size=256,batch_size=1,reward_scale=2):\n",
        "        os.makedirs('./tmp/sac', exist_ok=True)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.memory = ReplayBuffer(max_size, input_shape=input_shape, n_actions=n_actions,max_action=max_actions)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.max_actions= max_actions\n",
        "        self.scale = reward_scale\n",
        "\n",
        "        self.actor = ActorNetwork(alpha, input_shape,  n_actions=n_actions, name='actor', max_actions=self.max_actions)\n",
        "\n",
        "        self.critic_1 = CriticNetwork(beta, input_shape , n_actions=max_actions, name='critic_1')\n",
        "\n",
        "        self.critic_2 = CriticNetwork(beta, input_shape, n_actions=max_actions, name='critic_2')\n",
        "\n",
        "        self.value = ValueNetwork(beta, input_shape, name='value')\n",
        "\n",
        "        self.target_value = ValueNetwork(beta, input_shape, name='target_value')\n",
        "\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
        "        actions, log_probs = self.actor.sample_normal(state, self.max_actions)\n",
        "        return actions\n",
        "\n",
        "    def remember(self,state,action,reward,next_state,done):\n",
        "        self.memory.store_transition(state,action,reward,next_state,done)\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        target_value_params = self.target_value.named_parameters()\n",
        "        value_params = self.value.named_parameters()\n",
        "\n",
        "        target_value_state_dict = dict(target_value_params)\n",
        "        value_state_dict = dict(value_params)\n",
        "\n",
        "        for name in value_state_dict:\n",
        "            value_state_dict[name] = tau*value_state_dict[name].clone() + (1-tau)*target_value_state_dict[name].clone()\n",
        "\n",
        "        self.target_value.load_state_dict(value_state_dict)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('.... saving models ....')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.value.save_checkpoint()\n",
        "        self.target_value.save_checkpoint()\n",
        "        self.critic_1.save_checkpoint()\n",
        "        self.critic_2.save_checkpoint()\n",
        "\n",
        "\n",
        "    def load_models(self):\n",
        "        print('.... loading models ....')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.value.load_checkpoint()\n",
        "        self.target_value.load_checkpoint()\n",
        "        self.critic_1.load_checkpoint()\n",
        "        self.critic_2.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            print('not enough memories to learn from!')\n",
        "            return\n",
        "\n",
        "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
        "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
        "        done = T.tensor(done).to(self.actor.device)\n",
        "        state_ = T.tensor(new_state, dtype=T.float).to(self.actor.device)\n",
        "        state = T.tensor(state, dtype=T.float).to(self.actor.device)\n",
        "        action = T.tensor(action, dtype=T.float).to(self.actor.device)\n",
        "        flat_state = dr.flatten_nodes(state)\n",
        "        flat_state_= dr.flatten_nodes(state_)\n",
        "\n",
        "\n",
        "        value = self.value(flat_state).view(-1)\n",
        "        value_ = self.target_value(flat_state_).view(-1)\n",
        "        value_[done] = 0.0\n",
        "        actions, log_probs = self.actor.sample_normal(state, self.max_actions)\n",
        "        actions = actions[:self.max_actions]\n",
        "        actions = T.tensor(actions)\n",
        "        q1_new_policy = self.critic_1.forward(flat_state, actions)\n",
        "        q2_new_policy = self.critic_2.forward(flat_state, actions)\n",
        "        critic_value = T.min(q1_new_policy, q2_new_policy)\n",
        "        critic_value = critic_value.view(-1)\n",
        "        self.value.optimizer.zero_grad()\n",
        "        value_target = critic_value - log_probs\n",
        "        value_loss = 0.5 * F.mse_loss(value, value_target)\n",
        "        value_loss.backward(retain_graph=True)\n",
        "        self.value.optimizer.step()\n",
        "\n",
        "        log_probs = log_probs.view(-1)\n",
        "        q1_new_policy = self.critic_1.forward(flat_state, actions)\n",
        "        q2_new_policy = self.critic_2.forward(flat_state, actions)\n",
        "        critic_value = T.min(q1_new_policy, q2_new_policy)\n",
        "        critic_value = critic_value.view(-1)\n",
        "\n",
        "        actor_loss = log_probs - critic_value\n",
        "        actor_loss = T.mean(actor_loss)\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "        self.actor.optimizer.step()\n",
        "\n",
        "        self.critic_1.optimizer.zero_grad()\n",
        "        self.critic_2.optimizer.zero_grad()\n",
        "        q_hat = self.scale * reward + self.gamma * value_\n",
        "        q1_old_policy = self.critic_1.forward(flat_state, action).view(-1)\n",
        "        q2_old_policy = self.critic_2.forward(flat_state, action).view(-1)\n",
        "        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n",
        "        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n",
        "        critic_loss = critic_1_loss + critic_2_loss\n",
        "        critic_loss.backward()\n",
        "        self.critic_1.optimizer.step()\n",
        "        self.critic_2.optimizer.step()\n",
        "\n",
        "        self.update_network_parameters()\n",
        "        self.save_models()\n",
        "        print('updated the networks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izsWReg9yijZ",
        "outputId": "ea023f70-bb6a-48ea-9c34-8c2a8795cda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checpoint file tmp/sac/actor_sac\n",
            "in init value networks\n",
            "value network input dimensions 400\n",
            "in init value networks\n",
            "value network input dimensions 400\n",
            "added the transition 1\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 1\n",
            "the batch in sample ? [0]\n",
            "action from batch [[32.  1.  7. 30. 35. 13. 42.  6.  4. 27. 14. 36. 16. 38.  3. 15. 23.]]\n",
            "rewards from batch [0.]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 0.0\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(14)\n",
            "log probs tensor([ -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (42,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13, 14, 15, 16, 18, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 2\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 2\n",
            "the batch in sample ? [1]\n",
            "action from batch [[37.  1.  7. 30. 35. 46. 19.  8. 13. 42. 43.  6.  4. 49. 27. 36. 16.]]\n",
            "rewards from batch [565.03001]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 565.0300100000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725,\n",
            "         -0.6725])\n",
            "Sampled actions shape in learn: (45,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 3\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 3\n",
            "the batch in sample ? [1]\n",
            "action from batch [[37.  1.  7. 30. 35. 46. 19.  8. 13. 42. 43.  6.  4. 49. 27. 36. 16.]]\n",
            "rewards from batch [565.03001]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 565.0300100000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594, -15.9424,  -1.3594, -15.9424,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594, -15.9424,  -1.3594, -15.9424,\n",
            "         -1.3594,  -1.3594,  -1.3594,  -1.3594,  -1.3594, -15.9424,  -1.3594,\n",
            "         -1.3594])\n",
            "Sampled actions shape in learn: (45,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-46-3800f4382934>:86: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  done = T.tensor(done).to(self.actor.device)\n",
            "<ipython-input-83-8c57f130ec39>:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  selected_indices_tensor = T.tensor(selected_indices)\n",
            "<ipython-input-46-3800f4382934>:115: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  value_loss = 0.5 * F.mse_loss(value, value_target)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 4\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 4\n",
            "the batch in sample ? [0]\n",
            "action from batch [[32.  1.  7. 30. 35. 13. 42.  6.  4. 27. 14. 36. 16. 38.  3. 15. 23.]]\n",
            "rewards from batch [0.]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 0.0\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (42,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13, 14, 15, 16, 18, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 5\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 5\n",
            "the batch in sample ? [1]\n",
            "action from batch [[37.  1.  7. 30. 35. 46. 19.  8. 13. 42. 43.  6.  4. 49. 27. 36. 16.]]\n",
            "rewards from batch [565.03001]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 565.0300100000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725,\n",
            "         -0.6725])\n",
            "Sampled actions shape in learn: (45,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 6\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 6\n",
            "the batch in sample ? [0]\n",
            "action from batch [[32.  1.  7. 30. 35. 13. 42.  6.  4. 27. 14. 36. 16. 38.  3. 15. 23.]]\n",
            "rewards from batch [0.]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 0.0\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (42,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13, 14, 15, 16, 18, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 7\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 7\n",
            "the batch in sample ? [1]\n",
            "action from batch [[37.  1.  7. 30. 35. 46. 19.  8. 13. 42. 43.  6.  4. 49. 27. 36. 16.]]\n",
            "rewards from batch [565.03001]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 565.0300100000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725, -15.9424,\n",
            "         -0.6725,  -0.6725,  -0.6725,  -0.6725,  -0.6725, -15.9424,  -0.6725,\n",
            "         -0.6725])\n",
            "Sampled actions shape in learn: (45,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 8\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 8\n",
            "the batch in sample ? [4]\n",
            "action from batch [[32. 37. 30. 35. 19. 42. 46. 13. 43. 27. 49.  4. 36. 16. 38. 12. 15.]]\n",
            "rewards from batch [2588.39353]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 2588.39353\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(3)\n",
            "log probs tensor([ -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,\n",
            "         -1.3205,  -1.3205,  -1.3205,  -1.3205, -15.9424,  -1.3205,  -1.3205,\n",
            "         -1.3205,  -1.3205,  -1.3205, -15.9424,  -1.3205,  -1.3205,  -1.3205,\n",
            "         -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205, -15.9424,  -1.3205,\n",
            "        -15.9424,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,\n",
            "         -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,  -1.3205,\n",
            "         -1.3205,  -1.3205,  -1.3205, -15.9424,  -1.3205,  -1.3205, -15.9424,\n",
            "         -1.3205])\n",
            "Sampled actions shape in learn: (44,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 18])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 9\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 9\n",
            "the batch in sample ? [2]\n",
            "action from batch [[37. 30. 35. 19. 42. 49. 27.  1. 36. 16.  7. 38.  3. 12. 15. 23.  9.]]\n",
            "rewards from batch [576.74518]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 576.74518\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -0.6653,  -0.6653,  -0.6653,  -0.6653, -15.9424, -15.9424,  -0.6653,\n",
            "         -0.6653,  -0.6653,  -0.6653,  -0.6653,  -0.6653,  -0.6653, -15.9424,\n",
            "         -0.6653,  -0.6653,  -0.6653, -15.9424,  -0.6653,  -0.6653,  -0.6653,\n",
            "         -0.6653,  -0.6653,  -0.6653, -15.9424,  -0.6653, -15.9424,  -0.6653,\n",
            "         -0.6653,  -0.6653,  -0.6653,  -0.6653, -15.9424,  -0.6653,  -0.6653,\n",
            "         -0.6653,  -0.6653,  -0.6653,  -0.6653,  -0.6653, -15.9424,  -0.6653,\n",
            "         -0.6653, -15.9424, -15.9424,  -0.6653, -15.9424,  -0.6653,  -0.6653,\n",
            "         -0.6653])\n",
            "Sampled actions shape in learn: (39,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16, 18, 19, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 10\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 10\n",
            "the batch in sample ? [5]\n",
            "action from batch [[32. 30. 19. 46. 13. 43. 42. 49. 36. 16. 38. 15. 26. 29.  2. 45. 48.]]\n",
            "rewards from batch [1690.92208]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 1690.9220799999998\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830, -15.9424,\n",
            "         -1.3830, -15.9424, -15.9424,  -1.3830,  -1.3830, -15.9424,  -1.3830,\n",
            "         -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,\n",
            "         -1.3830, -15.9424, -15.9424,  -1.3830,  -1.3830,  -1.3830, -15.9424,\n",
            "         -1.3830,  -1.3830,  -1.3830, -15.9424,  -1.3830,  -1.3830,  -1.3830,\n",
            "        -15.9424,  -1.3830, -15.9424,  -1.3830,  -1.3830, -15.9424,  -1.3830,\n",
            "         -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,\n",
            "         -1.3830])\n",
            "Sampled actions shape in learn: (39,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  7, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 11\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 11\n",
            "the batch in sample ? [6]\n",
            "action from batch [[32. 30. 19. 35. 46. 13. 42. 49. 27. 38. 15. 23. 26. 29. 45. 48.  6.]]\n",
            "rewards from batch [1411.6518]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 1411.6517999999996\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,\n",
            "        -15.9424,  -0.6613, -15.9424,  -0.6613, -15.9424, -15.9424,  -0.6613,\n",
            "         -0.6613,  -0.6613, -15.9424,  -0.6613, -15.9424,  -0.6613,  -0.6613,\n",
            "         -0.6613,  -0.6613,  -0.6613,  -0.6613, -15.9424,  -0.6613,  -0.6613,\n",
            "         -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,\n",
            "         -0.6613, -15.9424, -15.9424,  -0.6613,  -0.6613,  -0.6613,  -0.6613,\n",
            "         -0.6613, -15.9424,  -0.6613,  -0.6613,  -0.6613,  -0.6613,  -0.6613,\n",
            "         -0.6613])\n",
            "Sampled actions shape in learn: (40,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 13, 14, 15, 17, 19, 20, 21, 22])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 12\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 12\n",
            "the batch in sample ? [4]\n",
            "action from batch [[32. 37. 30. 35. 19. 42. 46. 13. 43. 27. 49.  4. 36. 16. 38. 12. 15.]]\n",
            "rewards from batch [2588.39353]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 2588.39353\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,\n",
            "         -0.6718,  -0.6718,  -0.6718,  -0.6718, -15.9424,  -0.6718,  -0.6718,\n",
            "         -0.6718,  -0.6718,  -0.6718, -15.9424,  -0.6718,  -0.6718,  -0.6718,\n",
            "         -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718, -15.9424,  -0.6718,\n",
            "        -15.9424,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,\n",
            "         -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,  -0.6718,\n",
            "         -0.6718,  -0.6718,  -0.6718, -15.9424,  -0.6718,  -0.6718, -15.9424,\n",
            "         -0.6718])\n",
            "Sampled actions shape in learn: (44,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 18])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 13\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 13\n",
            "the batch in sample ? [0]\n",
            "action from batch [[32.  1.  7. 30. 35. 13. 42.  6.  4. 27. 14. 36. 16. 38.  3. 15. 23.]]\n",
            "rewards from batch [0.]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 0.0\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938, -15.9424,  -0.6938, -15.9424,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,  -0.6938,  -0.6938,\n",
            "         -0.6938, -15.9424,  -0.6938,  -0.6938, -15.9424,  -0.6938,  -0.6938,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (42,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13, 14, 15, 16, 18, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 14\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 14\n",
            "the batch in sample ? [3]\n",
            "action from batch [[32. 37. 30. 35. 19. 42. 13. 43. 27.  4. 36. 12. 15. 23.  9. 26.  5.]]\n",
            "rewards from batch [1137.60903]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 1137.6090299999998\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(14)\n",
            "log probs tensor([ -1.3599, -15.9424,  -1.3599,  -1.3599,  -1.3599,  -1.3599,  -1.3599,\n",
            "         -1.3599,  -1.3599,  -1.3599,  -1.3599, -15.9424,  -1.3599,  -1.3599,\n",
            "         -1.3599,  -1.3599, -15.9424,  -1.3599,  -1.3599,  -1.3599,  -1.3599,\n",
            "         -1.3599, -15.9424,  -1.3599,  -1.3599,  -1.3599,  -1.3599,  -1.3599,\n",
            "         -1.3599,  -1.3599,  -1.3599,  -1.3599,  -1.3599,  -1.3599,  -1.3599,\n",
            "         -1.3599,  -1.3599,  -1.3599, -15.9424, -15.9424,  -1.3599,  -1.3599,\n",
            "         -1.3599,  -1.3599,  -1.3599, -15.9424, -15.9424,  -1.3599,  -1.3599,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (41,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 17, 18, 19])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 15\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 15\n",
            "the batch in sample ? [13]\n",
            "action from batch [[30. 37. 19. 46. 13. 35. 43. 49. 16. 38. 36. 15. 12. 23. 26. 29. 45.]]\n",
            "rewards from batch [7265.12517]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 7265.125169999999\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "        -15.9424,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424, -15.9424,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,\n",
            "        -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370])\n",
            "Sampled actions shape in learn: (41,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 16\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 16\n",
            "the batch in sample ? [14]\n",
            "action from batch [[30. 32. 19. 46. 13. 35. 42. 27. 16. 38. 36. 12. 23. 26. 45. 48. 34.]]\n",
            "rewards from batch [7306.21294]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 7306.21294\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -2.7341, -15.9424,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,\n",
            "         -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,\n",
            "        -15.9424, -15.9424,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,\n",
            "         -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,  -2.7341,\n",
            "         -2.7341, -15.9424,  -2.7341,  -2.7341,  -2.7341, -15.9424,  -2.7341,\n",
            "         -2.7341,  -2.7341, -15.9424,  -2.7341,  -2.7341,  -2.7341,  -2.7341,\n",
            "         -2.7341, -15.9424,  -2.7341,  -2.7341,  -2.7341, -15.9424,  -2.7341,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (41,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16, 17, 18, 19])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 17\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 17\n",
            "the batch in sample ? [15]\n",
            "action from batch [[32. 37. 46. 35. 42. 27. 16. 49. 38. 15. 12. 23. 29. 48. 34.  7. 47.]]\n",
            "rewards from batch [5742.97529]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 5742.975289999999\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(3)\n",
            "log probs tensor([ -1.4263,  -1.4263, -15.9424,  -1.4263,  -1.4263,  -1.4263,  -1.4263,\n",
            "         -1.4263,  -1.4263,  -1.4263,  -1.4263,  -1.4263,  -1.4263, -15.9424,\n",
            "         -1.4263,  -1.4263,  -1.4263, -15.9424,  -1.4263, -15.9424,  -1.4263,\n",
            "         -1.4263,  -1.4263,  -1.4263,  -1.4263,  -1.4263, -15.9424,  -1.4263,\n",
            "        -15.9424,  -1.4263, -15.9424,  -1.4263,  -1.4263, -15.9424,  -1.4263,\n",
            "         -1.4263, -15.9424,  -1.4263,  -1.4263, -15.9424,  -1.4263,  -1.4263,\n",
            "         -1.4263, -15.9424,  -1.4263, -15.9424,  -1.4263,  -1.4263,  -1.4263,\n",
            "         -1.4263])\n",
            "Sampled actions shape in learn: (38,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16, 18, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 18\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 18\n",
            "the batch in sample ? [17]\n",
            "action from batch [[32. 46. 19. 35. 13. 42. 27. 49. 38. 36. 12. 26. 34. 33. 47. 39.  9.]]\n",
            "rewards from batch [5677.77407]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 5677.7740699999995\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(13)\n",
            "log probs tensor([ -1.3920,  -1.3920,  -1.3920,  -1.3920,  -1.3920,  -1.3920,  -1.3920,\n",
            "         -1.3920, -15.9424,  -1.3920,  -1.3920,  -1.3920,  -1.3920,  -1.3920,\n",
            "         -1.3920, -15.9424, -15.9424,  -1.3920,  -1.3920,  -1.3920, -15.9424,\n",
            "         -1.3920,  -1.3920, -15.9424,  -1.3920, -15.9424,  -1.3920,  -1.3920,\n",
            "         -1.3920, -15.9424, -15.9424, -15.9424,  -1.3920,  -1.3920,  -1.3920,\n",
            "         -1.3920,  -1.3920, -15.9424,  -1.3920,  -1.3920,  -1.3920,  -1.3920,\n",
            "         -1.3920, -15.9424,  -1.3920, -15.9424,  -1.3920,  -1.3920, -15.9424,\n",
            "         -1.3920])\n",
            "Sampled actions shape in learn: (37,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 17, 18, 19])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 19\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 19\n",
            "the batch in sample ? [18]\n",
            "action from batch [[32. 46. 30. 19. 37. 35. 13. 42. 27. 43. 16. 49. 38. 12. 15. 23. 26.]]\n",
            "rewards from batch [13250.84677]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 13250.84677\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278, -15.9424, -15.9424,\n",
            "         -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,\n",
            "         -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,\n",
            "         -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,\n",
            "         -0.6278, -15.9424,  -0.6278, -15.9424,  -0.6278,  -0.6278, -15.9424,\n",
            "         -0.6278, -15.9424,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,\n",
            "         -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,  -0.6278,\n",
            "         -0.6278])\n",
            "Sampled actions shape in learn: (44,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 20\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 20\n",
            "the batch in sample ? [19]\n",
            "action from batch [[30. 37. 35. 13. 42. 27. 16. 49. 38. 36. 23. 26. 45. 48. 34. 33. 47.]]\n",
            "rewards from batch [7743.05703]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 7743.057030000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(13)\n",
            "log probs tensor([ -1.4137,  -1.4137,  -1.4137,  -1.4137,  -1.4137,  -1.4137, -15.9424,\n",
            "         -1.4137,  -1.4137,  -1.4137, -15.9424, -15.9424, -15.9424,  -1.4137,\n",
            "         -1.4137, -15.9424,  -1.4137,  -1.4137,  -1.4137, -15.9424, -15.9424,\n",
            "         -1.4137,  -1.4137,  -1.4137,  -1.4137,  -1.4137,  -1.4137,  -1.4137,\n",
            "        -15.9424, -15.9424,  -1.4137,  -1.4137, -15.9424,  -1.4137,  -1.4137,\n",
            "         -1.4137,  -1.4137,  -1.4137,  -1.4137, -15.9424,  -1.4137,  -1.4137,\n",
            "         -1.4137, -15.9424,  -1.4137,  -1.4137, -15.9424,  -1.4137,  -1.4137,\n",
            "         -1.4137])\n",
            "Sampled actions shape in learn: (37,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 13, 14, 16, 17, 18, 21, 22, 23])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 21\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 21\n",
            "the batch in sample ? [12]\n",
            "action from batch [[32. 30. 19. 46. 13. 42. 27. 16. 38. 36. 15. 12. 23. 26. 45.  0.  1.]]\n",
            "rewards from batch [5486.08548]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 5486.08548\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -0.6271,  -0.6271,  -0.6271,  -0.6271, -15.9424, -15.9424, -15.9424,\n",
            "         -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,\n",
            "         -0.6271,  -0.6271,  -0.6271,  -0.6271, -15.9424,  -0.6271, -15.9424,\n",
            "        -15.9424, -15.9424,  -0.6271,  -0.6271, -15.9424,  -0.6271,  -0.6271,\n",
            "         -0.6271, -15.9424,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,\n",
            "        -15.9424,  -0.6271, -15.9424,  -0.6271,  -0.6271,  -0.6271, -15.9424,\n",
            "         -0.6271, -15.9424, -15.9424,  -0.6271,  -0.6271,  -0.6271, -15.9424,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (34,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 23])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 22\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 22\n",
            "the batch in sample ? [13]\n",
            "action from batch [[30. 37. 19. 46. 13. 35. 43. 49. 16. 38. 36. 15. 12. 23. 26. 29. 45.]]\n",
            "rewards from batch [7265.12517]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 7265.125169999999\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(3)\n",
            "log probs tensor([ -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "        -15.9424,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424, -15.9424,  -0.6370,\n",
            "         -0.6370,  -0.6370,  -0.6370,  -0.6370, -15.9424,  -0.6370,  -0.6370,\n",
            "        -15.9424,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,  -0.6370,\n",
            "         -0.6370])\n",
            "Sampled actions shape in learn: (41,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 23\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 23\n",
            "the batch in sample ? [4]\n",
            "action from batch [[32. 37. 30. 35. 19. 42. 46. 13. 43. 27. 49.  4. 36. 16. 38. 12. 15.]]\n",
            "rewards from batch [2588.39353]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 2588.39353\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,\n",
            "         -2.6287,  -2.6287,  -2.6287,  -2.6287, -15.9424,  -2.6287,  -2.6287,\n",
            "         -2.6287,  -2.6287,  -2.6287, -15.9424,  -2.6287,  -2.6287,  -2.6287,\n",
            "         -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287, -15.9424,  -2.6287,\n",
            "        -15.9424,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,\n",
            "         -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,  -2.6287,\n",
            "         -2.6287,  -2.6287,  -2.6287, -15.9424,  -2.6287,  -2.6287, -15.9424,\n",
            "         -2.6287])\n",
            "Sampled actions shape in learn: (44,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 18])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 24\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 24\n",
            "the batch in sample ? [23]\n",
            "action from batch [[46. 37. 32. 19. 30. 13. 27. 43. 49. 38. 36. 12. 23. 15. 26. 29. 45.]]\n",
            "rewards from batch [13861.55892]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 13861.558920000001\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([-15.9424,  -2.7516,  -2.7516,  -2.7516,  -2.7516,  -2.7516, -15.9424,\n",
            "         -2.7516,  -2.7516, -15.9424,  -2.7516,  -2.7516,  -2.7516,  -2.7516,\n",
            "         -2.7516,  -2.7516, -15.9424, -15.9424,  -2.7516,  -2.7516,  -2.7516,\n",
            "         -2.7516,  -2.7516,  -2.7516, -15.9424,  -2.7516,  -2.7516,  -2.7516,\n",
            "         -2.7516,  -2.7516,  -2.7516,  -2.7516,  -2.7516,  -2.7516,  -2.7516,\n",
            "        -15.9424,  -2.7516,  -2.7516,  -2.7516,  -2.7516, -15.9424,  -2.7516,\n",
            "        -15.9424,  -2.7516,  -2.7516,  -2.7516,  -2.7516, -15.9424, -15.9424,\n",
            "         -2.7516])\n",
            "Sampled actions shape in learn: (39,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 1,  2,  3,  4,  5,  7,  8, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 25\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 25\n",
            "the batch in sample ? [5]\n",
            "action from batch [[32. 30. 19. 46. 13. 43. 42. 49. 36. 16. 38. 15. 26. 29.  2. 45. 48.]]\n",
            "rewards from batch [1690.92208]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 1690.9220799999998\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(16)\n",
            "log probs tensor([ -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830, -15.9424,\n",
            "         -1.3830, -15.9424, -15.9424,  -1.3830,  -1.3830, -15.9424,  -1.3830,\n",
            "         -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,\n",
            "         -1.3830, -15.9424, -15.9424,  -1.3830,  -1.3830,  -1.3830, -15.9424,\n",
            "         -1.3830,  -1.3830,  -1.3830, -15.9424,  -1.3830,  -1.3830,  -1.3830,\n",
            "        -15.9424,  -1.3830, -15.9424,  -1.3830,  -1.3830, -15.9424,  -1.3830,\n",
            "         -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,  -1.3830,\n",
            "         -1.3830])\n",
            "Sampled actions shape in learn: (39,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  5,  7, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 26\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 26\n",
            "the batch in sample ? [18]\n",
            "action from batch [[32. 46. 30. 19. 37. 35. 13. 42. 27. 43. 16. 49. 38. 12. 15. 23. 26.]]\n",
            "rewards from batch [13250.84677]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 13250.84677\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534, -15.9424, -15.9424,\n",
            "         -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,\n",
            "         -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,\n",
            "         -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,\n",
            "         -2.5534, -15.9424,  -2.5534, -15.9424,  -2.5534,  -2.5534, -15.9424,\n",
            "         -2.5534, -15.9424,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,\n",
            "         -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,  -2.5534,\n",
            "         -2.5534])\n",
            "Sampled actions shape in learn: (44,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  4,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 27\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 27\n",
            "the batch in sample ? [7]\n",
            "action from batch [[32. 30. 37. 19. 35. 46. 13. 42. 43. 49. 27. 16. 38. 15. 23. 26. 29.]]\n",
            "rewards from batch [4115.73468]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 4115.73468\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([ -2.5836, -15.9424,  -2.5836,  -2.5836, -15.9424,  -2.5836,  -2.5836,\n",
            "         -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836, -15.9424,  -2.5836,\n",
            "        -15.9424,  -2.5836,  -2.5836, -15.9424,  -2.5836,  -2.5836,  -2.5836,\n",
            "         -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,\n",
            "         -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,\n",
            "         -2.5836, -15.9424,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,\n",
            "         -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836,  -2.5836, -15.9424,\n",
            "         -2.5836])\n",
            "Sampled actions shape in learn: (43,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  2,  3,  5,  6,  7,  8,  9, 10, 11, 13, 15, 16, 18, 19, 20, 21])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 28\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 28\n",
            "the batch in sample ? [10]\n",
            "action from batch [[32. 37. 30. 19. 35. 46. 13. 43. 27. 16. 38. 15. 12. 23. 26. 29. 45.]]\n",
            "rewards from batch [5488.58057]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 5488.580569999999\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(9)\n",
            "log probs tensor([-15.9424,  -2.9295,  -2.9295,  -2.9295,  -2.9295, -15.9424,  -2.9295,\n",
            "         -2.9295,  -2.9295,  -2.9295, -15.9424,  -2.9295,  -2.9295,  -2.9295,\n",
            "         -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,\n",
            "        -15.9424,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,\n",
            "         -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295, -15.9424,\n",
            "         -2.9295, -15.9424,  -2.9295,  -2.9295,  -2.9295,  -2.9295, -15.9424,\n",
            "        -15.9424,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,  -2.9295,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (41,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 1,  2,  3,  4,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n",
            "added the transition 29\n",
            "learning agent in learn\n",
            "batch_size 1\n",
            "maxmeme in sample buffer 29\n",
            "the batch in sample ? [12]\n",
            "action from batch [[32. 30. 19. 46. 13. 42. 27. 16. 38. 36. 15. 12. 23. 26. 45.  0.  1.]]\n",
            "rewards from batch [5486.08548]\n",
            "dones from batch [False]\n",
            "Sampled transitions: (50, 8) (17,) () (50, 8) ()\n",
            "done False\n",
            "reward 5486.08548\n",
            "made them tensors in leanr\n",
            "passed value\n",
            "passed other value\n",
            "value_done? tensor([], size=(0, 1), grad_fn=<IndexBackward0>)\n",
            "in sample_normal\n",
            "numselected in sample normal 17\n",
            "action probs in sample normazle torch.Size([17])\n",
            "action_dist from categorical  Categorical(probs: torch.Size([17]))\n",
            "sampled actions from categorical tensor(10)\n",
            "log probs tensor([ -0.6271,  -0.6271,  -0.6271,  -0.6271, -15.9424, -15.9424, -15.9424,\n",
            "         -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,\n",
            "         -0.6271,  -0.6271,  -0.6271,  -0.6271, -15.9424,  -0.6271, -15.9424,\n",
            "        -15.9424, -15.9424,  -0.6271,  -0.6271, -15.9424,  -0.6271,  -0.6271,\n",
            "         -0.6271, -15.9424,  -0.6271,  -0.6271,  -0.6271,  -0.6271,  -0.6271,\n",
            "        -15.9424,  -0.6271, -15.9424,  -0.6271,  -0.6271,  -0.6271, -15.9424,\n",
            "         -0.6271, -15.9424, -15.9424,  -0.6271,  -0.6271,  -0.6271, -15.9424,\n",
            "        -15.9424])\n",
            "Sampled actions shape in learn: (34,)\n",
            "truncated actions (17,)\n",
            "actions after tensor tensor([ 0,  1,  2,  3,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 23])\n",
            "done critic forward in learn\n",
            "zero grad value optimizer done\n",
            ".... saving models ....\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "...saving checkpoint...\n",
            "updated the networks\n"
          ]
        }
      ],
      "source": [
        "# import sys\n",
        "# sys.path.insert(0, './')  # Add the directory containing \"DRL\" to the path\n",
        "\n",
        "# from agent import node_selection_agent as ag\n",
        "# from environment import node_selection_env as nds\n",
        "\n",
        "total_nodes = 50\n",
        "max_actions = 17\n",
        "num_features = 8\n",
        "target_accuracy = 99.9\n",
        "max_rounds= 30\n",
        "#create the nodes selection environment accoding to the observation\n",
        "envNodeSelect = FLNodeSelectionEnv(total_nodes,max_actions, num_features=num_features, target=target_accuracy, max_rounds=max_rounds)\n",
        "obs_shape = envNodeSelect.observation_space.sample()[\"current_state\"].shape\n",
        "agent = Agent(input_shape=obs_shape ,n_actions=obs_shape[0], env=envNodeSelect,max_actions=max_actions)\n",
        "\n",
        "# train the agent with the transitions store the trasitions then make the agent learn\n",
        "for transition in transitions :\n",
        "    agent.remember(transition[\"obs\"],transition[\"action\"], transition[\"reward\"], transition[\"obs_\"], transition[\"done\"])\n",
        "    agent.learn()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
